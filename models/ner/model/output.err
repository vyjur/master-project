[nltk_data] Downloading package punkt to
[nltk_data]     /cluster/home/julievt/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/cluster/home/julievt/master-project/src/textmining/ner/setup.py:38: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  dataset['MedicalEntity'] = dataset['MedicalEntity'].fillna('O')
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: fop7bgor with config:
wandb: 	batch_size: 64
wandb: 	early_stopping_delta: 0.001
wandb: 	early_stopping_patience: 5
wandb: 	embedding_dim: 128
wandb: 	epochs: 100
wandb: 	learning_rate: 0.01
wandb: 	max_length: 512
wandb: 	num_workers: 0
wandb: 	optimizer: adam
wandb: 	shuffle: True
wandb: 	stride: 32
wandb: 	tune: true
wandb: 	valid_batch_size: 256
wandb: 	weight_decay: 0.0001
wandb: WARNING Ignoring project 'ner-b-Task.TOKEN-nn-model' when running a sweep.
wandb: Currently logged in as: julievt (julievt-ntnu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /cluster/home/julievt/master-project/wandb/run-20250215_105954-fop7bgor
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ethereal-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: üßπ View sweep at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/sweeps/2lu6oz6r
wandb: üöÄ View run at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/fop7bgor
  0%|          | 0/8 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 217, in train
    train_loss, train_acc = self.__train(
                            ^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 328, in __train
    outputs = self.__model(input_ids=ids, attention_mask=mask, labels=targets)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 493, in forward
    sequence_output, contextualized_embeddings, attention_probs = self.get_contextualized_embeddings(input_ids, attention_mask)
                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 294, in get_contextualized_embeddings
    contextualized_embeddings, attention_probs = self.transformer(static_embeddings, attention_mask, relative_embedding)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 41, in forward
    hidden_state, attention_p = layer(hidden_states[-1], attention_mask, relative_embedding)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 75, in forward
    attention_output, attention_probs = self.attention(x, padding_mask, relative_embedding)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 210, in forward
    return self.compute_output(attention_probs, value), attention_probs.detach()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 199, in compute_output
    attention_probs = self.dropout(attention_probs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 545.38 MiB is free. Including non-PyTorch memory, this process has 38.96 GiB memory in use. Of the allocated memory 37.81 GiB is allocated by PyTorch, and 673.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: üöÄ View run ethereal-sweep-1 at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/fop7bgor
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250215_105954-fop7bgor/logs
Run fop7bgor errored:
Traceback (most recent call last):
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 217, in train
    train_loss, train_acc = self.__train(
                            ^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 328, in __train
    outputs = self.__model(input_ids=ids, attention_mask=mask, labels=targets)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 493, in forward
    sequence_output, contextualized_embeddings, attention_probs = self.get_contextualized_embeddings(input_ids, attention_mask)
                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 294, in get_contextualized_embeddings
    contextualized_embeddings, attention_probs = self.transformer(static_embeddings, attention_mask, relative_embedding)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 41, in forward
    hidden_state, attention_p = layer(hidden_states[-1], attention_mask, relative_embedding)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 75, in forward
    attention_output, attention_probs = self.attention(x, padding_mask, relative_embedding)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 210, in forward
    return self.compute_output(attention_probs, value), attention_probs.detach()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 199, in compute_output
    attention_probs = self.dropout(attention_probs)
                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/dropout.py", line 59, in forward
    return F.dropout(input, self.p, self.training, self.inplace)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/functional.py", line 1295, in dropout
    return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
                                                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 545.38 MiB is free. Including non-PyTorch memory, this process has 38.96 GiB memory in use. Of the allocated memory 37.81 GiB is allocated by PyTorch, and 673.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run fop7bgor errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 217, in train
wandb: ERROR     train_loss, train_acc = self.__train(
wandb: ERROR                             ^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 328, in __train
wandb: ERROR     outputs = self.__model(input_ids=ids, attention_mask=mask, labels=targets)
wandb: ERROR               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 493, in forward
wandb: ERROR     sequence_output, contextualized_embeddings, attention_probs = self.get_contextualized_embeddings(input_ids, attention_mask)
wandb: ERROR                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 294, in get_contextualized_embeddings
wandb: ERROR     contextualized_embeddings, attention_probs = self.transformer(static_embeddings, attention_mask, relative_embedding)
wandb: ERROR                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 41, in forward
wandb: ERROR     hidden_state, attention_p = layer(hidden_states[-1], attention_mask, relative_embedding)
wandb: ERROR                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 75, in forward
wandb: ERROR     attention_output, attention_probs = self.attention(x, padding_mask, relative_embedding)
wandb: ERROR                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 210, in forward
wandb: ERROR     return self.compute_output(attention_probs, value), attention_probs.detach()
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 199, in compute_output
wandb: ERROR     attention_probs = self.dropout(attention_probs)
wandb: ERROR                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/dropout.py", line 59, in forward
wandb: ERROR     return F.dropout(input, self.p, self.training, self.inplace)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/functional.py", line 1295, in dropout
wandb: ERROR     return _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)
wandb: ERROR                                                             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 768.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 545.38 MiB is free. Including non-PyTorch memory, this process has 38.96 GiB memory in use. Of the allocated memory 37.81 GiB is allocated by PyTorch, and 673.98 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: u2smoo59 with config:
wandb: 	batch_size: 128
wandb: 	early_stopping_delta: 0.0005
wandb: 	early_stopping_patience: 3
wandb: 	embedding_dim: 256
wandb: 	epochs: 100
wandb: 	learning_rate: 0.001
wandb: 	max_length: 512
wandb: 	num_workers: 0
wandb: 	optimizer: adam
wandb: 	shuffle: True
wandb: 	stride: 16
wandb: 	tune: true
wandb: 	valid_batch_size: 256
wandb: 	weight_decay: 0.01
wandb: WARNING Ignoring project 'ner-b-Task.TOKEN-nn-model' when running a sweep.
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /cluster/home/julievt/master-project/wandb/run-20250215_110025-u2smoo59
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-sweep-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: üßπ View sweep at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/sweeps/2lu6oz6r
wandb: üöÄ View run at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/u2smoo59

  0%|          | 0/4 [00:00<?, ?it/s][ATraceback (most recent call last):
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 217, in train
    train_loss, train_acc = self.__train(
                            ^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 328, in __train
    outputs = self.__model(input_ids=ids, attention_mask=mask, labels=targets)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 493, in forward
    sequence_output, contextualized_embeddings, attention_probs = self.get_contextualized_embeddings(input_ids, attention_mask)
                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 294, in get_contextualized_embeddings
    contextualized_embeddings, attention_probs = self.transformer(static_embeddings, attention_mask, relative_embedding)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 41, in forward
    hidden_state, attention_p = layer(hidden_states[-1], attention_mask, relative_embedding)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 75, in forward
    attention_output, attention_probs = self.attention(x, padding_mask, relative_embedding)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 208, in forward
    attention_scores, value = self.compute_attention_scores(hidden_states, relative_embedding)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 171, in compute_attention_scores
    query, key = self.in_proj_qk(hidden_states).chunk(2, dim=2)  # shape: [T, B, D]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 217.38 MiB is free. Including non-PyTorch memory, this process has 39.28 GiB memory in use. Of the allocated memory 38.72 GiB is allocated by PyTorch, and 66.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: üöÄ View run cosmic-sweep-2 at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/u2smoo59
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250215_110025-u2smoo59/logs
Run u2smoo59 errored:
Traceback (most recent call last):
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 217, in train
    train_loss, train_acc = self.__train(
                            ^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 328, in __train
    outputs = self.__model(input_ids=ids, attention_mask=mask, labels=targets)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 493, in forward
    sequence_output, contextualized_embeddings, attention_probs = self.get_contextualized_embeddings(input_ids, attention_mask)
                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 294, in get_contextualized_embeddings
    contextualized_embeddings, attention_probs = self.transformer(static_embeddings, attention_mask, relative_embedding)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 41, in forward
    hidden_state, attention_p = layer(hidden_states[-1], attention_mask, relative_embedding)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 75, in forward
    attention_output, attention_probs = self.attention(x, padding_mask, relative_embedding)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 208, in forward
    attention_scores, value = self.compute_attention_scores(hidden_states, relative_embedding)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 171, in compute_attention_scores
    query, key = self.in_proj_qk(hidden_states).chunk(2, dim=2)  # shape: [T, B, D]
                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 117, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 217.38 MiB is free. Including non-PyTorch memory, this process has 39.28 GiB memory in use. Of the allocated memory 38.72 GiB is allocated by PyTorch, and 66.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run u2smoo59 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 217, in train
wandb: ERROR     train_loss, train_acc = self.__train(
wandb: ERROR                             ^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 328, in __train
wandb: ERROR     outputs = self.__model(input_ids=ids, attention_mask=mask, labels=targets)
wandb: ERROR               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 493, in forward
wandb: ERROR     sequence_output, contextualized_embeddings, attention_probs = self.get_contextualized_embeddings(input_ids, attention_mask)
wandb: ERROR                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 294, in get_contextualized_embeddings
wandb: ERROR     contextualized_embeddings, attention_probs = self.transformer(static_embeddings, attention_mask, relative_embedding)
wandb: ERROR                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 41, in forward
wandb: ERROR     hidden_state, attention_p = layer(hidden_states[-1], attention_mask, relative_embedding)
wandb: ERROR                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 75, in forward
wandb: ERROR     attention_output, attention_probs = self.attention(x, padding_mask, relative_embedding)
wandb: ERROR                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 208, in forward
wandb: ERROR     attention_scores, value = self.compute_attention_scores(hidden_states, relative_embedding)
wandb: ERROR                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 171, in compute_attention_scores
wandb: ERROR     query, key = self.in_proj_qk(hidden_states).chunk(2, dim=2)  # shape: [T, B, D]
wandb: ERROR                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/linear.py", line 117, in forward
wandb: ERROR     return F.linear(input, self.weight, self.bias)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 384.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 217.38 MiB is free. Including non-PyTorch memory, this process has 39.28 GiB memory in use. Of the allocated memory 38.72 GiB is allocated by PyTorch, and 66.32 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: f2w3s3tp with config:
wandb: 	batch_size: 64
wandb: 	early_stopping_delta: 0.0001
wandb: 	early_stopping_patience: 5
wandb: 	embedding_dim: 256
wandb: 	epochs: 100
wandb: 	learning_rate: 0.001
wandb: 	max_length: 256
wandb: 	num_workers: 0
wandb: 	optimizer: sgd
wandb: 	shuffle: True
wandb: 	stride: 0
wandb: 	tune: true
wandb: 	valid_batch_size: 256
wandb: 	weight_decay: 0.001
wandb: WARNING Ignoring project 'ner-b-Task.TOKEN-nn-model' when running a sweep.
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /cluster/home/julievt/master-project/wandb/run-20250215_110051-f2w3s3tp
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run eager-sweep-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: üßπ View sweep at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/sweeps/2lu6oz6r
wandb: üöÄ View run at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/f2w3s3tp
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
    self.__model.to(self.__device)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: üöÄ View run eager-sweep-3 at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/f2w3s3tp
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250215_110051-f2w3s3tp/logs
Run f2w3s3tp errored:
Traceback (most recent call last):
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
    self.__model.to(self.__device)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 3 more times]
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run f2w3s3tp errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
wandb: ERROR     self.__model.to(self.__device)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 3 more times]
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 12.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Sweep Agent: Waiting for job.
wandb: Job received.
wandb: Agent Starting Run: u84gh4f4 with config:
wandb: 	batch_size: 64
wandb: 	early_stopping_delta: 0.0001
wandb: 	early_stopping_patience: 5
wandb: 	embedding_dim: 512
wandb: 	epochs: 100
wandb: 	learning_rate: 0.1
wandb: 	max_length: 512
wandb: 	num_workers: 0
wandb: 	optimizer: adam
wandb: 	shuffle: True
wandb: 	stride: 32
wandb: 	tune: true
wandb: 	valid_batch_size: 256
wandb: 	weight_decay: 0.0001
wandb: WARNING Ignoring project 'ner-b-Task.TOKEN-nn-model' when running a sweep.
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /cluster/home/julievt/master-project/wandb/run-20250215_110117-u84gh4f4
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run glowing-sweep-4
wandb: ‚≠êÔ∏è View project at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: üßπ View sweep at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/sweeps/2lu6oz6r
wandb: üöÄ View run at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/u84gh4f4
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
    self.__model.to(self.__device)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: üöÄ View run glowing-sweep-4 at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/u84gh4f4
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250215_110117-u84gh4f4/logs
Run u84gh4f4 errored:
Traceback (most recent call last):
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
    self.__model.to(self.__device)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run u84gh4f4 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
wandb: ERROR     self.__model.to(self.__device)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: m6z6y72p with config:
wandb: 	batch_size: 64
wandb: 	early_stopping_delta: 0.001
wandb: 	early_stopping_patience: 5
wandb: 	embedding_dim: 256
wandb: 	epochs: 100
wandb: 	learning_rate: 0.1
wandb: 	max_length: 64
wandb: 	num_workers: 0
wandb: 	optimizer: sgd
wandb: 	shuffle: True
wandb: 	stride: 32
wandb: 	tune: true
wandb: 	valid_batch_size: 256
wandb: 	weight_decay: 0.0001
wandb: WARNING Ignoring project 'ner-b-Task.TOKEN-nn-model' when running a sweep.
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /cluster/home/julievt/master-project/wandb/run-20250215_110144-m6z6y72p
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run super-sweep-5
wandb: ‚≠êÔ∏è View project at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: üßπ View sweep at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/sweeps/2lu6oz6r
wandb: üöÄ View run at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/m6z6y72p
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
    self.__model.to(self.__device)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: üöÄ View run super-sweep-5 at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/m6z6y72p
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250215_110144-m6z6y72p/logs
Run m6z6y72p errored:
Traceback (most recent call last):
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
    self.__model.to(self.__device)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run m6z6y72p errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
wandb: ERROR     self.__model.to(self.__device)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: mfa6e12w with config:
wandb: 	batch_size: 128
wandb: 	early_stopping_delta: 0.01
wandb: 	early_stopping_patience: 10
wandb: 	embedding_dim: 512
wandb: 	epochs: 100
wandb: 	learning_rate: 0.001
wandb: 	max_length: 256
wandb: 	num_workers: 0
wandb: 	optimizer: sgd
wandb: 	shuffle: True
wandb: 	stride: 32
wandb: 	tune: true
wandb: 	valid_batch_size: 256
wandb: 	weight_decay: 0
wandb: WARNING Ignoring project 'ner-b-Task.TOKEN-nn-model' when running a sweep.
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /cluster/home/julievt/master-project/wandb/run-20250215_110200-mfa6e12w
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run earthy-sweep-6
wandb: ‚≠êÔ∏è View project at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: üßπ View sweep at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/sweeps/2lu6oz6r
wandb: üöÄ View run at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/mfa6e12w
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
    self.__model.to(self.__device)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: üöÄ View run earthy-sweep-6 at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/mfa6e12w
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250215_110200-mfa6e12w/logs
Run mfa6e12w errored:
Traceback (most recent call last):
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
    self.__model.to(self.__device)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run mfa6e12w errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
wandb: ERROR     self.__model.to(self.__device)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
Detected 5 failed runs in a row at start, killing sweep.
wandb: ERROR Detected 5 failed runs in a row at start, killing sweep.
wandb: To change this value set WANDB_AGENT_MAX_INITIAL_FAILURES=val
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/./scripts/train/ner/main.py", line 35, in <module>
    ner = NERecognition(
          ^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/textmining/ner/setup.py", line 74, in __init__
    self.__model = MODEL_MAP[self.__config["MODEL"]["name"]](
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/bert/token_bert.py", line 24, in __init__
    self.__bert = BERT(
                  ^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 127, in __init__
    model=self.__model.to(self.__device),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/./scripts/train/ner/main.py", line 35, in <module>
    ner = NERecognition(
          ^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/textmining/ner/setup.py", line 74, in __init__
    self.__model = MODEL_MAP[self.__config["MODEL"]["name"]](
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/bert/token_bert.py", line 24, in __init__
    self.__bert = BERT(
                  ^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 127, in __init__
    model=self.__model.to(self.__device),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/./scripts/train/ner/main.py", line 35, in <module>
    ner = NERecognition(
          ^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/textmining/ner/setup.py", line 74, in __init__
    self.__model = MODEL_MAP[self.__config["MODEL"]["name"]](
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/bert/token_bert.py", line 24, in __init__
    self.__bert = BERT(
                  ^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 127, in __init__
    model=self.__model.to(self.__device),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/./scripts/train/ner/main.py", line 35, in <module>
    ner = NERecognition(
          ^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/textmining/ner/setup.py", line 74, in __init__
    self.__model = MODEL_MAP[self.__config["MODEL"]["name"]](
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/bert/token_bert.py", line 24, in __init__
    self.__bert = BERT(
                  ^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 127, in __init__
    model=self.__model.to(self.__device),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/./scripts/train/ner/main.py", line 35, in <module>
    ner = NERecognition(
          ^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/textmining/ner/setup.py", line 74, in __init__
    self.__model = MODEL_MAP[self.__config["MODEL"]["name"]](
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/bert/token_bert.py", line 24, in __init__
    self.__bert = BERT(
                  ^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 127, in __init__
    model=self.__model.to(self.__device),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/./scripts/train/ner/main.py", line 35, in <module>
    ner = NERecognition(
          ^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/textmining/ner/setup.py", line 74, in __init__
    self.__model = MODEL_MAP[self.__config["MODEL"]["name"]](
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/bert/token_bert.py", line 24, in __init__
    self.__bert = BERT(
                  ^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 127, in __init__
    model=self.__model.to(self.__device),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/./scripts/train/ner/main.py", line 35, in <module>
    ner = NERecognition(
          ^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/textmining/ner/setup.py", line 74, in __init__
    self.__model = MODEL_MAP[self.__config["MODEL"]["name"]](
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/bert/token_bert.py", line 24, in __init__
    self.__bert = BERT(
                  ^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 127, in __init__
    model=self.__model.to(self.__device),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 5.38 MiB is free. Including non-PyTorch memory, this process has 39.48 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 61.10 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
