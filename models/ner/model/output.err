[nltk_data] Downloading package punkt to
[nltk_data]     /cluster/home/julievt/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/cluster/home/julievt/master-project/src/textmining/ner/setup.py:38: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  dataset['MedicalEntity'] = dataset['MedicalEntity'].fillna('O')
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Agent Starting Run: wq18o2gf with config:
wandb: 	batch_size: 256
wandb: 	early_stopping_delta: 0.0001
wandb: 	early_stopping_patience: 10
wandb: 	embedding_dim: 256
wandb: 	epochs: 100
wandb: 	learning_rate: 0.001
wandb: 	max_length: 128
wandb: 	num_workers: 0
wandb: 	optimizer: adam
wandb: 	shuffle: True
wandb: 	stride: 0
wandb: 	tune: true
wandb: 	valid_batch_size: 256
wandb: 	weight_decay: 0
wandb: WARNING Ignoring project 'ner-b-Task.TOKEN-nn-model' when running a sweep.
wandb: Currently logged in as: julievt (julievt-ntnu). Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /cluster/home/julievt/master-project/wandb/run-20250215_102025-wq18o2gf
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run wild-sweep-1
wandb: ‚≠êÔ∏è View project at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: üßπ View sweep at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/sweeps/89h2jafy
wandb: üöÄ View run at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/wq18o2gf
  0%|          | 0/7 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 217, in train
    train_loss, train_acc = self.__train(
                            ^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 328, in __train
    outputs = self.__model(input_ids=ids, attention_mask=mask, labels=targets)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 493, in forward
    sequence_output, contextualized_embeddings, attention_probs = self.get_contextualized_embeddings(input_ids, attention_mask)
                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 294, in get_contextualized_embeddings
    contextualized_embeddings, attention_probs = self.transformer(static_embeddings, attention_mask, relative_embedding)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 41, in forward
    hidden_state, attention_p = layer(hidden_states[-1], attention_mask, relative_embedding)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 75, in forward
    attention_output, attention_probs = self.attention(x, padding_mask, relative_embedding)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 208, in forward
    attention_scores, value = self.compute_attention_scores(hidden_states, relative_embedding)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 190, in compute_attention_scores
    attention_p_c = attention_p_c.gather(2, position_indices)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 163.38 MiB is free. Including non-PyTorch memory, this process has 39.33 GiB memory in use. Of the allocated memory 38.79 GiB is allocated by PyTorch, and 51.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: üöÄ View run wild-sweep-1 at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/wq18o2gf
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250215_102025-wq18o2gf/logs
Run wq18o2gf errored:
Traceback (most recent call last):
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 217, in train
    train_loss, train_acc = self.__train(
                            ^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 328, in __train
    outputs = self.__model(input_ids=ids, attention_mask=mask, labels=targets)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 493, in forward
    sequence_output, contextualized_embeddings, attention_probs = self.get_contextualized_embeddings(input_ids, attention_mask)
                                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 294, in get_contextualized_embeddings
    contextualized_embeddings, attention_probs = self.transformer(static_embeddings, attention_mask, relative_embedding)
                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 41, in forward
    hidden_state, attention_p = layer(hidden_states[-1], attention_mask, relative_embedding)
                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 75, in forward
    attention_output, attention_probs = self.attention(x, padding_mask, relative_embedding)
                                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 208, in forward
    attention_scores, value = self.compute_attention_scores(hidden_states, relative_embedding)
                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 190, in compute_attention_scores
    attention_p_c = attention_p_c.gather(2, position_indices)
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 163.38 MiB is free. Including non-PyTorch memory, this process has 39.33 GiB memory in use. Of the allocated memory 38.79 GiB is allocated by PyTorch, and 51.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run wq18o2gf errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 217, in train
wandb: ERROR     train_loss, train_acc = self.__train(
wandb: ERROR                             ^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 328, in __train
wandb: ERROR     outputs = self.__model(input_ids=ids, attention_mask=mask, labels=targets)
wandb: ERROR               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 493, in forward
wandb: ERROR     sequence_output, contextualized_embeddings, attention_probs = self.get_contextualized_embeddings(input_ids, attention_mask)
wandb: ERROR                                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 294, in get_contextualized_embeddings
wandb: ERROR     contextualized_embeddings, attention_probs = self.transformer(static_embeddings, attention_mask, relative_embedding)
wandb: ERROR                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 41, in forward
wandb: ERROR     hidden_state, attention_p = layer(hidden_states[-1], attention_mask, relative_embedding)
wandb: ERROR                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 75, in forward
wandb: ERROR     attention_output, attention_probs = self.attention(x, padding_mask, relative_embedding)
wandb: ERROR                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
wandb: ERROR     return self._call_impl(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
wandb: ERROR     return forward_call(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 208, in forward
wandb: ERROR     attention_scores, value = self.compute_attention_scores(hidden_states, relative_embedding)
wandb: ERROR                               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/huggingface/modules/transformers_modules/ltg/norbert3-base/4376f702588d56cd29276a183582f77345d77a4e/modeling_norbert.py", line 190, in compute_attention_scores
wandb: ERROR     attention_p_c = attention_p_c.gather(2, position_indices)
wandb: ERROR                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 192.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 163.38 MiB is free. Including non-PyTorch memory, this process has 39.33 GiB memory in use. Of the allocated memory 38.79 GiB is allocated by PyTorch, and 51.62 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: i20e0b9n with config:
wandb: 	batch_size: 256
wandb: 	early_stopping_delta: 0.0001
wandb: 	early_stopping_patience: 3
wandb: 	embedding_dim: 32
wandb: 	epochs: 100
wandb: 	learning_rate: 0.01
wandb: 	max_length: 256
wandb: 	num_workers: 0
wandb: 	optimizer: sgd
wandb: 	shuffle: True
wandb: 	stride: 60
wandb: 	tune: true
wandb: 	valid_batch_size: 256
wandb: 	weight_decay: 0.01
wandb: WARNING Ignoring project 'ner-b-Task.TOKEN-nn-model' when running a sweep.
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /cluster/home/julievt/master-project/wandb/run-20250215_102046-i20e0b9n
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run magic-sweep-2
wandb: ‚≠êÔ∏è View project at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: üßπ View sweep at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/sweeps/89h2jafy
wandb: üöÄ View run at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/i20e0b9n
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
    self.__model.to(self.__device)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 854, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 43.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: üöÄ View run magic-sweep-2 at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/i20e0b9n
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250215_102046-i20e0b9n/logs
Run i20e0b9n errored:
Traceback (most recent call last):
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
    self.__model.to(self.__device)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  [Previous line repeated 1 more time]
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 854, in _apply
    self._buffers[key] = fn(buf)
                         ^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 43.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run i20e0b9n errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
wandb: ERROR     self.__model.to(self.__device)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   [Previous line repeated 1 more time]
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 854, in _apply
wandb: ERROR     self._buffers[key] = fn(buf)
wandb: ERROR                          ^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 43.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
wandb: Agent Starting Run: ptab66y2 with config:
wandb: 	batch_size: 512
wandb: 	early_stopping_delta: 0.001
wandb: 	early_stopping_patience: 5
wandb: 	embedding_dim: 128
wandb: 	epochs: 100
wandb: 	learning_rate: 0.01
wandb: 	max_length: 128
wandb: 	num_workers: 0
wandb: 	optimizer: adam
wandb: 	shuffle: True
wandb: 	stride: 60
wandb: 	tune: true
wandb: 	valid_batch_size: 256
wandb: 	weight_decay: 0.01
wandb: WARNING Ignoring project 'ner-b-Task.TOKEN-nn-model' when running a sweep.
wandb: Tracking run with wandb version 0.19.1
wandb: Run data is saved locally in /cluster/home/julievt/master-project/wandb/run-20250215_102107-ptab66y2
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run winter-sweep-3
wandb: ‚≠êÔ∏è View project at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: üßπ View sweep at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/sweeps/89h2jafy
wandb: üöÄ View run at https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/ptab66y2
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
    self.__model.to(self.__device)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 43.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb:                                                                                
wandb: üöÄ View run winter-sweep-3 at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model/runs/ptab66y2
wandb: ‚≠êÔ∏è View project at: https://wandb.ai/julievt-ntnu/ner-b-Task.TOKEN-bert-model
wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20250215_102107-ptab66y2/logs
Run ptab66y2 errored:
Traceback (most recent call last):
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
    self._function()
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
    self.__model.to(self.__device)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 43.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)

wandb: ERROR Run ptab66y2 errored:
wandb: ERROR Traceback (most recent call last):
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/wandb/agents/pyagent.py", line 306, in _run_job
wandb: ERROR     self._function()
wandb: ERROR   File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 184, in train
wandb: ERROR     self.__model.to(self.__device)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
wandb: ERROR     return super().to(*args, **kwargs)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
wandb: ERROR     return self._apply(convert)
wandb: ERROR            ^^^^^^^^^^^^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
wandb: ERROR     module._apply(fn)
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
wandb: ERROR     param_applied = fn(param)
wandb: ERROR                     ^^^^^^^^^
wandb: ERROR   File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
wandb: ERROR     return t.to(
wandb: ERROR            ^^^^^
wandb: ERROR torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 43.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
wandb: ERROR 
Detected 3 failed runs in the first 60 seconds, killing sweep.
wandb: ERROR Detected 3 failed runs in the first 60 seconds, killing sweep.
wandb: To disable this check set WANDB_AGENT_DISABLE_FLAPPING=true
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/./scripts/train/ner/main.py", line 35, in <module>
    ner = NERecognition(
          ^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/textmining/ner/setup.py", line 74, in __init__
    self.__model = MODEL_MAP[self.__config["MODEL"]["name"]](
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/bert/token_bert.py", line 24, in __init__
    self.__bert = BERT(
                  ^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 127, in __init__
    model=self.__model.to(self.__device),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 43.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/./scripts/train/ner/main.py", line 35, in <module>
    ner = NERecognition(
          ^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/textmining/ner/setup.py", line 74, in __init__
    self.__model = MODEL_MAP[self.__config["MODEL"]["name"]](
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/bert/token_bert.py", line 24, in __init__
    self.__bert = BERT(
                  ^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 127, in __init__
    model=self.__model.to(self.__device),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 43.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/./scripts/train/ner/main.py", line 35, in <module>
    ner = NERecognition(
          ^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/textmining/ner/setup.py", line 74, in __init__
    self.__model = MODEL_MAP[self.__config["MODEL"]["name"]](
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/bert/token_bert.py", line 24, in __init__
    self.__bert = BERT(
                  ^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 127, in __init__
    model=self.__model.to(self.__device),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 43.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/cluster/home/julievt/master-project/./scripts/train/ner/main.py", line 35, in <module>
    ner = NERecognition(
          ^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/textmining/ner/setup.py", line 74, in __init__
    self.__model = MODEL_MAP[self.__config["MODEL"]["name"]](
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/master-project/src/model/bert/token_bert.py", line 24, in __init__
    self.__bert = BERT(
                  ^^^^^
  File "/cluster/home/julievt/master-project/src/model/base/bert.py", line 127, in __init__
    model=self.__model.to(self.__device),
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/transformers/modeling_utils.py", line 2958, in to
    return super().to(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1174, in to
    return self._apply(convert)
           ^^^^^^^^^^^^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 780, in _apply
    module._apply(fn)
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 805, in _apply
    param_applied = fn(param)
                    ^^^^^^^^^
  File "/cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py", line 1160, in convert
    return t.to(
           ^^^^^
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 148.00 MiB. GPU 0 has a total capacity of 39.50 GiB of which 15.38 MiB is free. Including non-PyTorch memory, this process has 39.47 GiB memory in use. Of the allocated memory 38.94 GiB is allocated by PyTorch, and 43.30 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
