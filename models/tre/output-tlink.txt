Virtual environment already activated: /cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12
Installing dependencies from lock file

No dependencies to install or update

Installing the current project: master-project (0.1.0)
##### Start training for TRE... ######
### Processing the files:
###### (0) Training for configuration file: c-bert-bilstm.ini
LOAD False
Using: cuda:0 with NN
{'OVERLAP': 11844, 'BEFORE': 3137, 'O': 3563}
{'OVERLAP': 0.5218957559383092, 'BEFORE': 1.9704600998831154, 'O': 1.7348676209187015}
Epoch 1
-------------------------------
Batch 0, Loss: 1.0822951793670654
Batch 100, Loss: 1.1084014177322388
Batch 200, Loss: 1.018539547920227
Batch 300, Loss: 1.0963208675384521
Epoch 2
-------------------------------
Batch 0, Loss: 1.1058541536331177
Batch 100, Loss: 1.1641169786453247
Batch 200, Loss: 1.0713163614273071
Batch 300, Loss: 1.171514630317688
Epoch 3
-------------------------------
Batch 0, Loss: 1.1474494934082031
Batch 100, Loss: 1.008654236793518
Batch 200, Loss: 1.2165814638137817
Batch 300, Loss: 1.1488877534866333
Epoch 4
-------------------------------
Batch 0, Loss: 1.2394920587539673
Batch 100, Loss: 1.046973705291748
Batch 200, Loss: 1.110724687576294
Batch 300, Loss: 1.3870484828948975
Epoch 5
-------------------------------
Batch 0, Loss: 1.132964849472046
Batch 100, Loss: 1.177720308303833
Batch 200, Loss: 1.0965877771377563
Batch 300, Loss: 1.135480284690857
Epoch 6
-------------------------------
Batch 0, Loss: 1.0916337966918945
Batch 100, Loss: 1.1035665273666382
Batch 200, Loss: 1.138970136642456
Batch 300, Loss: 1.2367732524871826
Epoch 7
-------------------------------
Batch 0, Loss: 1.113167643547058
Batch 100, Loss: 1.1135082244873047
Batch 200, Loss: 1.1187466382980347
Batch 300, Loss: 1.1565642356872559
Early stopping
### Valid set performance:
Validation Accuracy: 0.18231566820276496
### BIO-Scheme
              precision    recall  f1-score   support

      BEFORE       0.00      0.00      0.00       509
           O       0.18      1.00      0.31       540
     OVERLAP       0.00      0.00      0.00      1918

    accuracy                           0.18      2967
   macro avg       0.06      0.33      0.10      2967
weighted avg       0.03      0.18      0.06      2967

### Summary
              precision    recall  f1-score   support

     OVERLAP       0.00      0.00      0.00      1918
           O       0.18      1.00      0.31       540
      BEFORE       0.00      0.00      0.00       509

    accuracy                           0.18      2967
   macro avg       0.06      0.33      0.10      2967
weighted avg       0.03      0.18      0.06      2967

### Test set performance:
Validation Accuracy: 0.1856763925729443
### BIO-Scheme
              precision    recall  f1-score   support

      BEFORE       0.00      0.00      0.00       611
           O       0.19      1.00      0.31       689
     OVERLAP       0.00      0.00      0.00      2409

    accuracy                           0.19      3709
   macro avg       0.06      0.33      0.10      3709
weighted avg       0.03      0.19      0.06      3709

### Summary
              precision    recall  f1-score   support

     OVERLAP       0.00      0.00      0.00      2409
           O       0.19      1.00      0.31       689
      BEFORE       0.00      0.00      0.00       611

    accuracy                           0.19      3709
   macro avg       0.06      0.33      0.10      3709
weighted avg       0.03      0.19      0.06      3709

Finished with this task.
###### (2) Training for configuration file: b-bert.ini
LOAD False
Using: cuda:0 with BERT
{'OVERLAP': 11844, 'BEFORE': 3137, 'O': 3470}
{'OVERLAP': 0.5192783969379714, 'BEFORE': 1.960578046966316, 'O': 1.7724303554274736}
Training Epoch: 0
Training loss epoch: 1.1557786464691162
Training accuracy epoch: 0.3296917344173442
Training Epoch: 1
Training loss epoch: 1.1098852157592773
Training accuracy epoch: 0.3279979674796748
Training Epoch: 2
Training loss epoch: 1.1068617105484009
Training accuracy epoch: 0.36348238482384826
Training Epoch: 3
Training loss epoch: 1.1088671684265137
Training accuracy epoch: 0.360010162601626
Training Epoch: 4
Training loss epoch: 1.1078792810440063
Training accuracy epoch: 0.3319783197831978
Training Epoch: 5
Training loss epoch: 1.1080427169799805
Training accuracy epoch: 0.34273373983739835
Training Epoch: 6
Training loss epoch: 1.110654354095459
Training accuracy epoch: 0.33765243902439024
Early stopping
### Valid set performance:
Validation loss per 100 evaluation steps: 1.103829264640808
Validation loss per 100 evaluation steps: 1.091620673047434
Validation Loss: 1.088572822390376
Validation Accuracy: 0.1608108108108108
### BIO-Scheme
              precision    recall  f1-score   support

      BEFORE       0.16      1.00      0.28       476
           O       0.00      0.00      0.00       577
     OVERLAP       0.00      0.00      0.00      1899

    accuracy                           0.16      2952
   macro avg       0.05      0.33      0.09      2952
weighted avg       0.03      0.16      0.04      2952

### Summary
              precision    recall  f1-score   support

     OVERLAP       0.00      0.00      0.00      1899
           O       0.00      0.00      0.00       577
      BEFORE       0.16      1.00      0.28       476

    accuracy                           0.16      2952
   macro avg       0.05      0.33      0.09      2952
weighted avg       0.03      0.16      0.04      2952

### Test set performance:
Validation loss per 100 evaluation steps: 1.0546482801437378
Validation loss per 100 evaluation steps: 1.08389413828897
Validation loss per 100 evaluation steps: 1.082433598551584
Validation Loss: 1.0827188027369512
Validation Accuracy: 0.17544765840220386
### BIO-Scheme
              precision    recall  f1-score   support

      BEFORE       0.18      1.00      0.30       648
           O       0.00      0.00      0.00       675
     OVERLAP       0.00      0.00      0.00      2368

    accuracy                           0.18      3691
   macro avg       0.06      0.33      0.10      3691
weighted avg       0.03      0.18      0.05      3691

### Summary
              precision    recall  f1-score   support

     OVERLAP       0.00      0.00      0.00      2368
           O       0.00      0.00      0.00       675
      BEFORE       0.18      1.00      0.30       648

    accuracy                           0.18      3691
   macro avg       0.06      0.33      0.10      3691
weighted avg       0.03      0.18      0.05      3691

Finished with this task.
###### (3) Training for configuration file: a-bilstm.ini
LOAD False
Using: cuda:0 with NN
{'OVERLAP': 11844, 'BEFORE': 3137, 'O': 3278}
{'OVERLAP': 0.5138748170663064, 'BEFORE': 1.9401763893316333, 'O': 1.8567215781980884}
Epoch 1
-------------------------------
Batch 0, Loss: 1.104399561882019
Batch 100, Loss: 1.0816266536712646
Batch 200, Loss: 0.9043643474578857
Batch 300, Loss: 1.0676733255386353
Epoch 2
-------------------------------
Batch 0, Loss: 1.019552230834961
Batch 100, Loss: 0.9705252051353455
Batch 200, Loss: 0.9889625310897827
Batch 300, Loss: 0.9793648719787598
Epoch 3
-------------------------------
Batch 0, Loss: 1.0199836492538452
Batch 100, Loss: 0.9892187118530273
Batch 200, Loss: 1.0332292318344116
Batch 300, Loss: 0.8639873266220093
Epoch 4
-------------------------------
Batch 0, Loss: 1.0265341997146606
Batch 100, Loss: 0.8115741610527039
Batch 200, Loss: 0.8491034507751465
Batch 300, Loss: 0.7958817481994629
Epoch 5
-------------------------------
Batch 0, Loss: 0.8467735052108765
Batch 100, Loss: 0.7943553328514099
Batch 200, Loss: 0.7977182269096375
Batch 300, Loss: 0.7535121440887451
Epoch 6
-------------------------------
Batch 0, Loss: 0.8502118587493896
Batch 100, Loss: 0.659676194190979
Batch 200, Loss: 0.8584261536598206
Batch 300, Loss: 0.8954938054084778
Epoch 7
-------------------------------
Batch 0, Loss: 0.8008095026016235
Batch 100, Loss: 0.8165265917778015
Batch 200, Loss: 0.9551242589950562
Batch 300, Loss: 0.8176039457321167
Epoch 8
-------------------------------
Batch 0, Loss: 0.8379102945327759
Batch 100, Loss: 0.9228283166885376
Batch 200, Loss: 0.8698215484619141
Batch 300, Loss: 0.7960622310638428
Epoch 9
-------------------------------
Batch 0, Loss: 0.8085429072380066
Batch 100, Loss: 0.9587189555168152
Batch 200, Loss: 0.9351850152015686
Batch 300, Loss: 1.0073543787002563
Epoch 10
-------------------------------
Batch 0, Loss: 0.6532960534095764
Batch 100, Loss: 0.8470419049263
Batch 200, Loss: 0.924099862575531
Batch 300, Loss: 0.7763214111328125
Epoch 11
-------------------------------
Batch 0, Loss: 0.8956629633903503
Batch 100, Loss: 0.8128516674041748
Batch 200, Loss: 0.7455072402954102
Batch 300, Loss: 0.8255894184112549
Epoch 12
-------------------------------
Batch 0, Loss: 0.8190189599990845
Batch 100, Loss: 0.721649169921875
Batch 200, Loss: 0.7088169455528259
Batch 300, Loss: 0.8987160325050354
Epoch 13
-------------------------------
Batch 0, Loss: 0.7596930861473083
Batch 100, Loss: 0.7174918055534363
Batch 200, Loss: 0.9454197287559509
Batch 300, Loss: 0.829997181892395
Epoch 14
-------------------------------
Batch 0, Loss: 0.7946667671203613
Batch 100, Loss: 0.7375363111495972
Batch 200, Loss: 0.9801682829856873
Batch 300, Loss: 0.9541319012641907
Epoch 15
-------------------------------
Batch 0, Loss: 0.7025123834609985
Batch 100, Loss: 1.00395667552948
Batch 200, Loss: 0.9455738067626953
Batch 300, Loss: 1.0374222993850708
Epoch 16
-------------------------------
Batch 0, Loss: 0.7038814425468445
Batch 100, Loss: 0.8938195705413818
Batch 200, Loss: 0.8067843914031982
Batch 300, Loss: 0.8624523282051086
Epoch 17
-------------------------------
Batch 0, Loss: 0.711610734462738
Batch 100, Loss: 0.6469981074333191
Batch 200, Loss: 0.8495392799377441
Batch 300, Loss: 0.7861620783805847
Epoch 18
-------------------------------
Batch 0, Loss: 0.9276737570762634
Batch 100, Loss: 0.7615920305252075
Batch 200, Loss: 0.689712643623352
Batch 300, Loss: 0.7442169189453125
Epoch 19
-------------------------------
Batch 0, Loss: 0.6823503375053406
Batch 100, Loss: 0.7559403777122498
Batch 200, Loss: 0.8337830305099487
Batch 300, Loss: 0.6911850571632385
Epoch 20
-------------------------------
Batch 0, Loss: 0.8110596537590027
Batch 100, Loss: 0.8809194564819336
Batch 200, Loss: 0.8283655643463135
Batch 300, Loss: 0.966395914554596
Epoch 21
-------------------------------
Batch 0, Loss: 0.6345577836036682
Batch 100, Loss: 0.7755504846572876
Batch 200, Loss: 0.8413254618644714
Batch 300, Loss: 0.8225986361503601
Epoch 22
-------------------------------
Batch 0, Loss: 0.6223621368408203
Batch 100, Loss: 0.5962197780609131
Batch 200, Loss: 0.8722575902938843
Batch 300, Loss: 0.7590950131416321
Epoch 23
-------------------------------
Batch 0, Loss: 0.78487628698349
Batch 100, Loss: 0.7757611870765686
Batch 200, Loss: 0.7423338890075684
Batch 300, Loss: 0.7353014349937439
Epoch 24
-------------------------------
Batch 0, Loss: 0.6931583285331726
Batch 100, Loss: 1.089751124382019
Batch 200, Loss: 0.9249534010887146
Batch 300, Loss: 0.7727620601654053
Epoch 25
-------------------------------
Batch 0, Loss: 0.6667593121528625
Batch 100, Loss: 0.8361949324607849
Batch 200, Loss: 0.961837649345398
Batch 300, Loss: 0.655799925327301
Epoch 26
-------------------------------
Batch 0, Loss: 0.7333167791366577
Batch 100, Loss: 0.5091963410377502
Batch 200, Loss: 0.7089607119560242
Batch 300, Loss: 0.8242045640945435
Epoch 27
-------------------------------
Batch 0, Loss: 0.7362671494483948
Batch 100, Loss: 0.6327176094055176
Batch 200, Loss: 0.9833585023880005
Batch 300, Loss: 0.648082971572876
Epoch 28
-------------------------------
Batch 0, Loss: 0.6660671830177307
Batch 100, Loss: 0.9463661313056946
Batch 200, Loss: 0.798363983631134
Batch 300, Loss: 0.6448546648025513
Epoch 29
-------------------------------
Batch 0, Loss: 0.6229300498962402
Batch 100, Loss: 0.7935447096824646
Batch 200, Loss: 0.8644838333129883
Batch 300, Loss: 0.8352685570716858
Epoch 30
-------------------------------
Batch 0, Loss: 0.6142492890357971
Batch 100, Loss: 0.7390583753585815
Batch 200, Loss: 0.6652154326438904
Batch 300, Loss: 0.9095584750175476
Epoch 31
-------------------------------
Batch 0, Loss: 0.7414643168449402
Batch 100, Loss: 0.6479570865631104
Batch 200, Loss: 0.7102217078208923
Batch 300, Loss: 0.7478089928627014
Epoch 32
-------------------------------
Batch 0, Loss: 0.9089688658714294
Batch 100, Loss: 0.8238554000854492
Batch 200, Loss: 0.6458737850189209
Batch 300, Loss: 0.720014750957489
Epoch 33
-------------------------------
Batch 0, Loss: 0.78163081407547
Batch 100, Loss: 0.6387001872062683
Batch 200, Loss: 0.7590070962905884
Batch 300, Loss: 0.7482749819755554
Epoch 34
-------------------------------
Batch 0, Loss: 0.704408586025238
Batch 100, Loss: 1.0391266345977783
Batch 200, Loss: 0.664038896560669
Batch 300, Loss: 0.7810540199279785
Epoch 35
-------------------------------
Batch 0, Loss: 0.6497106552124023
Batch 100, Loss: 0.5816303491592407
Batch 200, Loss: 0.6511385440826416
Batch 300, Loss: 0.6172564625740051
Epoch 36
-------------------------------
Batch 0, Loss: 0.5758385062217712
Batch 100, Loss: 0.7142535448074341
Batch 200, Loss: 0.7786274552345276
Batch 300, Loss: 0.8804519772529602
Epoch 37
-------------------------------
Batch 0, Loss: 0.7569445371627808
Batch 100, Loss: 0.6197828054428101
Batch 200, Loss: 0.7847265601158142
Batch 300, Loss: 0.7588671445846558
Epoch 38
-------------------------------
Batch 0, Loss: 0.686864972114563
Batch 100, Loss: 0.8909042477607727
Batch 200, Loss: 0.6852664947509766
Batch 300, Loss: 0.9421842694282532
Epoch 39
-------------------------------
Batch 0, Loss: 0.7200563549995422
Batch 100, Loss: 0.9765758514404297
Batch 200, Loss: 0.8425176739692688
Batch 300, Loss: 0.9028271436691284
Epoch 40
-------------------------------
Batch 0, Loss: 0.859908938407898
Batch 100, Loss: 0.8653817176818848
Batch 200, Loss: 0.5916842818260193
Batch 300, Loss: 0.659428060054779
Epoch 41
-------------------------------
Batch 0, Loss: 0.6784523129463196
Batch 100, Loss: 0.7999985814094543
Batch 200, Loss: 0.7826164960861206
Batch 300, Loss: 0.6000073552131653
Epoch 42
-------------------------------
Batch 0, Loss: 0.8114773035049438
Batch 100, Loss: 0.7614306807518005
Batch 200, Loss: 0.8219814300537109
Batch 300, Loss: 0.7800008058547974
Epoch 43
-------------------------------
Batch 0, Loss: 0.7951316237449646
Batch 100, Loss: 0.7844188809394836
Batch 200, Loss: 0.6752139925956726
Batch 300, Loss: 0.721258819103241
Epoch 44
-------------------------------
Batch 0, Loss: 0.6715943217277527
Batch 100, Loss: 0.7378961443901062
Batch 200, Loss: 0.7989309430122375
Batch 300, Loss: 0.6629325151443481
Epoch 45
-------------------------------
Batch 0, Loss: 0.6854184865951538
Batch 100, Loss: 0.8148770928382874
Batch 200, Loss: 0.7770530581474304
Batch 300, Loss: 0.8605211973190308
Epoch 46
-------------------------------
Batch 0, Loss: 0.631811261177063
Batch 100, Loss: 0.6771470904350281
Batch 200, Loss: 0.8862455487251282
Batch 300, Loss: 0.7312517762184143
Epoch 47
-------------------------------
Batch 0, Loss: 0.7342258095741272
Batch 100, Loss: 0.6852543354034424
Batch 200, Loss: 0.6758108139038086
Batch 300, Loss: 0.6156440377235413
Epoch 48
-------------------------------
Batch 0, Loss: 0.8145440816879272
Batch 100, Loss: 0.9532483220100403
Batch 200, Loss: 0.7106065154075623
Batch 300, Loss: 0.6367585062980652
Epoch 49
-------------------------------
Batch 0, Loss: 0.5578207969665527
Batch 100, Loss: 0.6208389401435852
Batch 200, Loss: 0.9285198450088501
Batch 300, Loss: 0.9020355343818665
Epoch 50
-------------------------------
Batch 0, Loss: 0.8367545008659363
Batch 100, Loss: 0.691186249256134
Batch 200, Loss: 0.689243495464325
Batch 300, Loss: 0.6481082439422607
Epoch 51
-------------------------------
Batch 0, Loss: 0.7338111400604248
Batch 100, Loss: 0.6876941919326782
Batch 200, Loss: 0.6058553457260132
Batch 300, Loss: 0.873293399810791
Epoch 52
-------------------------------
Batch 0, Loss: 0.6976778507232666
Batch 100, Loss: 0.6564084887504578
Batch 200, Loss: 0.6223147511482239
Batch 300, Loss: 0.7510802149772644
Epoch 53
-------------------------------
Batch 0, Loss: 0.7370206117630005
Batch 100, Loss: 0.7129327058792114
Batch 200, Loss: 0.9023693203926086
Batch 300, Loss: 0.8386571407318115
Epoch 54
-------------------------------
Batch 0, Loss: 0.4813137352466583
Batch 100, Loss: 0.5575556755065918
Batch 200, Loss: 0.8831399083137512
Batch 300, Loss: 0.7988916635513306
Epoch 55
-------------------------------
Batch 0, Loss: 0.5495801568031311
Batch 100, Loss: 0.7351671457290649
Batch 200, Loss: 0.7112460732460022
Batch 300, Loss: 0.7256866693496704
Epoch 56
-------------------------------
Batch 0, Loss: 0.6998991370201111
Batch 100, Loss: 0.7563208937644958
Batch 200, Loss: 0.5491183996200562
Batch 300, Loss: 0.591722846031189
Epoch 57
-------------------------------
Batch 0, Loss: 0.6078837513923645
Batch 100, Loss: 0.721605122089386
Batch 200, Loss: 0.8900841474533081
Batch 300, Loss: 0.6572547554969788
Epoch 58
-------------------------------
Batch 0, Loss: 0.77433180809021
Batch 100, Loss: 0.7303017973899841
Batch 200, Loss: 0.6421831250190735
Batch 300, Loss: 0.7198576331138611
Epoch 59
-------------------------------
Batch 0, Loss: 0.6784070134162903
Batch 100, Loss: 0.5719217658042908
Batch 200, Loss: 0.7989118099212646
Batch 300, Loss: 0.8269147276878357
Early stopping
### Valid set performance:
Validation Accuracy: 0.4824453551912568
### BIO-Scheme
              precision    recall  f1-score   support

      BEFORE       0.30      0.67      0.41       498
           O       0.35      0.49      0.41       513
     OVERLAP       0.75      0.43      0.55      1911

    accuracy                           0.48      2922
   macro avg       0.47      0.53      0.46      2922
weighted avg       0.61      0.48      0.50      2922

### Summary
              precision    recall  f1-score   support

      BEFORE       0.30      0.67      0.41       498
     OVERLAP       0.75      0.43      0.55      1911
           O       0.35      0.49      0.41       513

    accuracy                           0.48      2922
   macro avg       0.47      0.53      0.46      2922
weighted avg       0.61      0.48      0.50      2922

### Test set performance:
Validation Accuracy: 0.47079694323144106
### BIO-Scheme
              precision    recall  f1-score   support

      BEFORE       0.30      0.70      0.42       632
           O       0.32      0.41      0.36       668
     OVERLAP       0.75      0.43      0.54      2352

    accuracy                           0.47      3652
   macro avg       0.46      0.51      0.44      3652
weighted avg       0.59      0.47      0.49      3652

### Summary
              precision    recall  f1-score   support

      BEFORE       0.30      0.70      0.42       632
     OVERLAP       0.75      0.43      0.54      2352
           O       0.32      0.41      0.36       668

    accuracy                           0.47      3652
   macro avg       0.46      0.51      0.44      3652
weighted avg       0.59      0.47      0.49      3652

Finished with this task.
Process finished!
