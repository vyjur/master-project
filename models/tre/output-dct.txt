Spawning shell within /cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12
Installing dependencies from lock file

No dependencies to install or update

Installing the current project: master-project (0.1.0)
##### Start training for TRE... ######
### Processing the files:
###### (0) Training for configuration file: c-bert-bilstm.ini
LOAD False
Using: cuda:0 with NN
{'BEFORE': 13827, 'OVERLAP': 4119, 'BEFOREOVERLAP': 493, 'AFTER': 1113}
{'BEFORE': 0.3535112461126781, 'OVERLAP': 1.1866957999514445, 'BEFOREOVERLAP': 9.914807302231237, 'AFTER': 4.391734052111411}
Epoch 1
-------------------------------
Batch 0, Loss: 1.349182367324829
Batch 100, Loss: 0.7745867371559143
Batch 200, Loss: 1.1121826171875
Batch 300, Loss: 0.987084686756134
Epoch 2
-------------------------------
Batch 0, Loss: 0.916398286819458
Batch 100, Loss: 1.0088443756103516
Batch 200, Loss: 0.9034562706947327
Batch 300, Loss: 0.9929444193840027
Epoch 3
-------------------------------
Batch 0, Loss: 0.915860652923584
Batch 100, Loss: 0.9932805299758911
Batch 200, Loss: 0.8214380145072937
Batch 300, Loss: 0.8987720012664795
Epoch 4
-------------------------------
Batch 0, Loss: 1.0088660717010498
Batch 100, Loss: 1.636103868484497
Batch 200, Loss: 1.178847312927246
Batch 300, Loss: 0.892967939376831
Epoch 5
-------------------------------
Batch 0, Loss: 1.1703773736953735
Batch 100, Loss: 1.0189526081085205
Batch 200, Loss: 1.4372875690460205
Batch 300, Loss: 1.0544953346252441
Epoch 6
-------------------------------
Batch 0, Loss: 1.2062281370162964
Batch 100, Loss: 0.8558582663536072
Batch 200, Loss: 1.314903974533081
Batch 300, Loss: 1.0554758310317993
Epoch 7
-------------------------------
Batch 0, Loss: 1.143161416053772
Batch 100, Loss: 0.8090355396270752
Batch 200, Loss: 0.9552623629570007
Batch 300, Loss: 0.8044671416282654
Epoch 8
-------------------------------
Batch 0, Loss: 0.9143555760383606
Batch 100, Loss: 1.1454929113388062
Batch 200, Loss: 0.9519665241241455
Batch 300, Loss: 0.8673981428146362
Epoch 9
-------------------------------
Batch 0, Loss: 1.0676873922348022
Batch 100, Loss: 1.1358698606491089
Batch 200, Loss: 0.8624401092529297
Batch 300, Loss: 1.1562587022781372
Early stopping
### Valid set performance:
Validation Accuracy: 0.21892715419501133
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       176
       BEFORE       0.00      0.00      0.00      2193
BEFOREOVERLAP       0.00      0.00      0.00        75
      OVERLAP       0.22      1.00      0.36       685

     accuracy                           0.22      3129
    macro avg       0.05      0.25      0.09      3129
 weighted avg       0.05      0.22      0.08      3129

### Summary
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       176
BEFOREOVERLAP       0.00      0.00      0.00        75
      OVERLAP       0.22      1.00      0.36       685
       BEFORE       0.00      0.00      0.00      2193

     accuracy                           0.22      3129
    macro avg       0.05      0.25      0.09      3129
 weighted avg       0.05      0.22      0.08      3129

### Test set performance:
Validation Accuracy: 0.2135204081632653
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       236
       BEFORE       0.00      0.00      0.00      2741
BEFOREOVERLAP       0.00      0.00      0.00        97
      OVERLAP       0.21      1.00      0.35       837

     accuracy                           0.21      3911
    macro avg       0.05      0.25      0.09      3911
 weighted avg       0.05      0.21      0.08      3911

### Summary
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       236
BEFOREOVERLAP       0.00      0.00      0.00        97
      OVERLAP       0.21      1.00      0.35       837
       BEFORE       0.00      0.00      0.00      2741

     accuracy                           0.21      3911
    macro avg       0.05      0.25      0.09      3911
 weighted avg       0.05      0.21      0.08      3911

Finished with this task.
###### (2) Training for configuration file: b-bert.ini
LOAD False
Using: cuda:0 with BERT
{'BEFORE': 13827, 'OVERLAP': 4119, 'BEFOREOVERLAP': 493, 'AFTER': 1113}
{'BEFORE': 0.3535112461126781, 'OVERLAP': 1.1866957999514445, 'BEFOREOVERLAP': 9.914807302231237, 'AFTER': 4.391734052111411}
Training Epoch: 0
Training loss epoch: 1.0854650735855103
Training accuracy epoch: 0.43925831202046034
Training Epoch: 1
Training loss epoch: 1.0427626371383667
Training accuracy epoch: 0.4390185421994885
Training Epoch: 2
Training loss epoch: 1.0335955619812012
Training accuracy epoch: 0.45460358056265987
Training Epoch: 3
Training loss epoch: 1.0349836349487305
Training accuracy epoch: 0.4287084398976982
Training Epoch: 4
Training loss epoch: 1.0307328701019287
Training accuracy epoch: 0.4211157289002558
Training Epoch: 5
Training loss epoch: 1.0402883291244507
Training accuracy epoch: 0.441096547314578
Training Epoch: 6
Training loss epoch: 1.0377521514892578
Training accuracy epoch: 0.4358216112531969
Training Epoch: 7
Training loss epoch: 1.039823055267334
Training accuracy epoch: 0.4321451406649616
Training Epoch: 8
Training loss epoch: 1.0296685695648193
Training accuracy epoch: 0.4359814578005115
Training Epoch: 9
Training loss epoch: 1.0404025316238403
Training accuracy epoch: 0.4428548593350384
Early stopping
### Valid set performance:
Validation loss per 100 evaluation steps: 0.7233051061630249
Validation loss per 100 evaluation steps: 1.0334552603192848
Validation Loss: 1.0137092826925977
Validation Accuracy: 0.7007865646258503
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       176
       BEFORE       0.70      1.00      0.82      2193
BEFOREOVERLAP       0.00      0.00      0.00        75
      OVERLAP       0.00      0.00      0.00       685

     accuracy                           0.70      3129
    macro avg       0.18      0.25      0.21      3129
 weighted avg       0.49      0.70      0.58      3129

### Summary
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       176
BEFOREOVERLAP       0.00      0.00      0.00        75
      OVERLAP       0.00      0.00      0.00       685
       BEFORE       0.70      1.00      0.82      2193

     accuracy                           0.70      3129
    macro avg       0.18      0.25      0.21      3129
 weighted avg       0.49      0.70      0.58      3129

### Test set performance:
Validation loss per 100 evaluation steps: 1.938416600227356
Validation loss per 100 evaluation steps: 1.055225113240799
Validation loss per 100 evaluation steps: 1.0401072659302706
Validation Loss: 1.0312159752359196
Validation Accuracy: 0.701530612244898
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       236
       BEFORE       0.70      1.00      0.82      2741
BEFOREOVERLAP       0.00      0.00      0.00        97
      OVERLAP       0.00      0.00      0.00       837

     accuracy                           0.70      3911
    macro avg       0.18      0.25      0.21      3911
 weighted avg       0.49      0.70      0.58      3911

### Summary
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       236
BEFOREOVERLAP       0.00      0.00      0.00        97
      OVERLAP       0.00      0.00      0.00       837
       BEFORE       0.70      1.00      0.82      2741

     accuracy                           0.70      3911
    macro avg       0.18      0.25      0.21      3911
 weighted avg       0.49      0.70      0.58      3911

Finished with this task.
###### (3) Training for configuration file: a-bilstm.ini
LOAD False
Using: cuda:0 with NN
{'BEFORE': 13827, 'OVERLAP': 4119, 'BEFOREOVERLAP': 493, 'AFTER': 1113}
{'BEFORE': 0.3535112461126781, 'OVERLAP': 1.1866957999514445, 'BEFOREOVERLAP': 9.914807302231237, 'AFTER': 4.391734052111411}
Epoch 1
-------------------------------
Batch 0, Loss: 1.3991336822509766
Batch 100, Loss: 0.7520536780357361
Batch 200, Loss: 0.8132168650627136
Batch 300, Loss: 0.8225376605987549
Epoch 2
-------------------------------
Batch 0, Loss: 1.4538944959640503
Batch 100, Loss: 0.6812022924423218
Batch 200, Loss: 0.5472733974456787
Batch 300, Loss: 0.8317786455154419
Epoch 3
-------------------------------
Batch 0, Loss: 0.6310215592384338
Batch 100, Loss: 0.8452013731002808
Batch 200, Loss: 0.6345746517181396
Batch 300, Loss: 0.6876295804977417
Epoch 4
-------------------------------
Batch 0, Loss: 0.30027487874031067
Batch 100, Loss: 0.39800599217414856
Batch 200, Loss: 1.1072434186935425
Batch 300, Loss: 0.632649302482605
Epoch 5
-------------------------------
Batch 0, Loss: 0.5283979177474976
Batch 100, Loss: 0.48063528537750244
Batch 200, Loss: 0.8268226981163025
Batch 300, Loss: 0.8023215532302856
Epoch 6
-------------------------------
Batch 0, Loss: 0.505517840385437
Batch 100, Loss: 0.2128058224916458
Batch 200, Loss: 0.2097981870174408
Batch 300, Loss: 0.4263359606266022
Epoch 7
-------------------------------
Batch 0, Loss: 0.4505491554737091
Batch 100, Loss: 0.601318359375
Batch 200, Loss: 0.42586830258369446
Batch 300, Loss: 1.0438125133514404
Epoch 8
-------------------------------
Batch 0, Loss: 0.6472700238227844
Batch 100, Loss: 0.2634718120098114
Batch 200, Loss: 0.6240921020507812
Batch 300, Loss: 0.42105525732040405
Epoch 9
-------------------------------
Batch 0, Loss: 0.37142065167427063
Batch 100, Loss: 0.2863927185535431
Batch 200, Loss: 0.35393738746643066
Batch 300, Loss: 0.4040534794330597
Epoch 10
-------------------------------
Batch 0, Loss: 0.35538628697395325
Batch 100, Loss: 0.19561955332756042
Batch 200, Loss: 0.1683325469493866
Batch 300, Loss: 0.4686349034309387
Epoch 11
-------------------------------
Batch 0, Loss: 0.3505731523036957
Batch 100, Loss: 0.13978509604930878
Batch 200, Loss: 0.22033068537712097
Batch 300, Loss: 0.2983403205871582
Epoch 12
-------------------------------
Batch 0, Loss: 0.11455989629030228
Batch 100, Loss: 0.18577732145786285
Batch 200, Loss: 0.5667715072631836
Batch 300, Loss: 0.3330027163028717
Epoch 13
-------------------------------
Batch 0, Loss: 0.12569667398929596
Batch 100, Loss: 0.37096425890922546
Batch 200, Loss: 0.3463303744792938
Batch 300, Loss: 0.4132623076438904
Epoch 14
-------------------------------
Batch 0, Loss: 0.1857580840587616
Batch 100, Loss: 0.3375466465950012
Batch 200, Loss: 0.3843405544757843
Batch 300, Loss: 0.5006160736083984
Epoch 15
-------------------------------
Batch 0, Loss: 0.14840933680534363
Batch 100, Loss: 0.23057883977890015
Batch 200, Loss: 0.2868196368217468
Batch 300, Loss: 0.2869337797164917
Epoch 16
-------------------------------
Batch 0, Loss: 0.09800352156162262
Batch 100, Loss: 0.06972155719995499
Batch 200, Loss: 0.12302105128765106
Batch 300, Loss: 0.3078248202800751
Epoch 17
-------------------------------
Batch 0, Loss: 0.12520645558834076
Batch 100, Loss: 0.21530550718307495
Batch 200, Loss: 0.35543563961982727
Batch 300, Loss: 0.43720534443855286
Epoch 18
-------------------------------
Batch 0, Loss: 0.08349142223596573
Batch 100, Loss: 0.25340521335601807
Batch 200, Loss: 0.14401192963123322
Batch 300, Loss: 0.2176651805639267
Epoch 19
-------------------------------
Batch 0, Loss: 0.2494439035654068
Batch 100, Loss: 0.19288042187690735
Batch 200, Loss: 0.26707956194877625
Batch 300, Loss: 0.09874717146158218
Epoch 20
-------------------------------
Batch 0, Loss: 0.26354148983955383
Batch 100, Loss: 0.2347039133310318
Batch 200, Loss: 0.13833989202976227
Batch 300, Loss: 0.20393775403499603
Epoch 21
-------------------------------
Batch 0, Loss: 0.21160218119621277
Batch 100, Loss: 0.21277391910552979
Batch 200, Loss: 0.12510378658771515
Batch 300, Loss: 0.14182892441749573
Epoch 22
-------------------------------
Batch 0, Loss: 0.2019263207912445
Batch 100, Loss: 0.08786963671445847
Batch 200, Loss: 0.17811881005764008
Batch 300, Loss: 0.10574261844158173
Epoch 23
-------------------------------
Batch 0, Loss: 0.17425844073295593
Batch 100, Loss: 0.17548319697380066
Batch 200, Loss: 0.1125025600194931
Batch 300, Loss: 0.4190160930156708
Epoch 24
-------------------------------
Batch 0, Loss: 0.12833164632320404
Batch 100, Loss: 0.12856200337409973
Batch 200, Loss: 0.0823671743273735
Batch 300, Loss: 0.3171740770339966
Epoch 25
-------------------------------
Batch 0, Loss: 0.1654829978942871
Batch 100, Loss: 0.22045503556728363
Batch 200, Loss: 0.18012188374996185
Batch 300, Loss: 0.14762844145298004
Epoch 26
-------------------------------
Batch 0, Loss: 0.14348070323467255
Batch 100, Loss: 0.05577987805008888
Batch 200, Loss: 0.11591153591871262
Batch 300, Loss: 0.06586592644453049
Epoch 27
-------------------------------
Batch 0, Loss: 0.2120952308177948
Batch 100, Loss: 0.10494391620159149
Batch 200, Loss: 0.8414888978004456
Batch 300, Loss: 0.12997907400131226
Epoch 28
-------------------------------
Batch 0, Loss: 0.19473543763160706
Batch 100, Loss: 0.062203582376241684
Batch 200, Loss: 0.10398397594690323
Batch 300, Loss: 0.09924595057964325
Epoch 29
-------------------------------
Batch 0, Loss: 0.2770126760005951
Batch 100, Loss: 0.11715151369571686
Batch 200, Loss: 0.2531646192073822
Batch 300, Loss: 0.3455156981945038
Epoch 30
-------------------------------
Batch 0, Loss: 0.11649948358535767
Batch 100, Loss: 0.23330527544021606
Batch 200, Loss: 0.1824902445077896
Batch 300, Loss: 0.11639228463172913
Epoch 31
-------------------------------
Batch 0, Loss: 0.0994548425078392
Batch 100, Loss: 0.21313165128231049
Batch 200, Loss: 0.15725530683994293
Batch 300, Loss: 0.08539600670337677
Epoch 32
-------------------------------
Batch 0, Loss: 0.1912543624639511
Batch 100, Loss: 0.1687638908624649
Batch 200, Loss: 0.41498470306396484
Batch 300, Loss: 0.033971987664699554
Epoch 33
-------------------------------
Batch 0, Loss: 0.05636031925678253
Batch 100, Loss: 0.1329871118068695
Batch 200, Loss: 0.07272389531135559
Batch 300, Loss: 0.07430582493543625
Epoch 34
-------------------------------
Batch 0, Loss: 0.08049451559782028
Batch 100, Loss: 0.22674593329429626
Batch 200, Loss: 0.16186046600341797
Batch 300, Loss: 0.12108694016933441
Epoch 35
-------------------------------
Batch 0, Loss: 0.061839859932661057
Batch 100, Loss: 0.12626859545707703
Batch 200, Loss: 0.06927622854709625
Batch 300, Loss: 0.0929674506187439
Epoch 36
-------------------------------
Batch 0, Loss: 0.12761755287647247
Batch 100, Loss: 0.22890780866146088
Batch 200, Loss: 0.14244283735752106
Batch 300, Loss: 0.3597258925437927
Epoch 37
-------------------------------
Batch 0, Loss: 0.06208285316824913
Batch 100, Loss: 0.09377791732549667
Batch 200, Loss: 0.1278037130832672
Batch 300, Loss: 0.22875665128231049
Epoch 38
-------------------------------
Batch 0, Loss: 0.33488693833351135
Batch 100, Loss: 0.1183672621846199
Batch 200, Loss: 0.2515776753425598
Batch 300, Loss: 0.047072041779756546
Epoch 39
-------------------------------
Batch 0, Loss: 0.06324216723442078
Batch 100, Loss: 0.06272401660680771
Batch 200, Loss: 0.19535577297210693
Batch 300, Loss: 0.06010119616985321
Epoch 40
-------------------------------
Batch 0, Loss: 0.14139129221439362
Batch 100, Loss: 0.15316013991832733
Batch 200, Loss: 0.0702211931347847
Batch 300, Loss: 0.05999492108821869
Epoch 41
-------------------------------
Batch 0, Loss: 0.1212269514799118
Batch 100, Loss: 0.03726451098918915
Batch 200, Loss: 0.5084835290908813
Batch 300, Loss: 0.20548376441001892
Epoch 42
-------------------------------
Batch 0, Loss: 0.049518883228302
Batch 100, Loss: 0.06248731166124344
Batch 200, Loss: 0.06483390182256699
Batch 300, Loss: 0.3730914294719696
Epoch 43
-------------------------------
Batch 0, Loss: 0.03657623752951622
Batch 100, Loss: 0.08145051449537277
Batch 200, Loss: 0.08410006761550903
Batch 300, Loss: 0.13159827888011932
Epoch 44
-------------------------------
Batch 0, Loss: 0.037573475390672684
Batch 100, Loss: 0.11040345579385757
Batch 200, Loss: 0.11916118115186691
Batch 300, Loss: 0.3325672745704651
Epoch 45
-------------------------------
Batch 0, Loss: 0.11473684757947922
Batch 100, Loss: 0.05456953123211861
Batch 200, Loss: 0.10178709030151367
Batch 300, Loss: 0.10399701446294785
Epoch 46
-------------------------------
Batch 0, Loss: 0.15901800990104675
Batch 100, Loss: 0.020306894555687904
Batch 200, Loss: 0.1450357288122177
Batch 300, Loss: 0.31102901697158813
Epoch 47
-------------------------------
Batch 0, Loss: 0.021518360823392868
Batch 100, Loss: 0.09007789939641953
Batch 200, Loss: 0.15549075603485107
Batch 300, Loss: 0.1839907467365265
Epoch 48
-------------------------------
Batch 0, Loss: 0.2572803199291229
Batch 100, Loss: 0.3581465780735016
Batch 200, Loss: 0.09102095663547516
Batch 300, Loss: 0.09705211967229843
Epoch 49
-------------------------------
Batch 0, Loss: 0.25070443749427795
Batch 100, Loss: 0.0844719186425209
Batch 200, Loss: 0.13180504739284515
Batch 300, Loss: 0.30499595403671265
Epoch 50
-------------------------------
Batch 0, Loss: 0.69950932264328
Batch 100, Loss: 0.5220872759819031
Batch 200, Loss: 0.10668081045150757
Batch 300, Loss: 0.20786631107330322
Epoch 51
-------------------------------
Batch 0, Loss: 0.24395179748535156
Batch 100, Loss: 0.07202339172363281
Batch 200, Loss: 0.1795707792043686
Batch 300, Loss: 0.3970073163509369
Epoch 52
-------------------------------
Batch 0, Loss: 0.038232408463954926
Batch 100, Loss: 0.0785931721329689
Batch 200, Loss: 0.15129724144935608
Batch 300, Loss: 0.08512905985116959
Epoch 53
-------------------------------
Batch 0, Loss: 0.023497290909290314
Batch 100, Loss: 0.11136922240257263
Batch 200, Loss: 0.045973509550094604
Batch 300, Loss: 0.06474938243627548
Epoch 54
-------------------------------
Batch 0, Loss: 0.2164933830499649
Batch 100, Loss: 0.04279184341430664
Batch 200, Loss: 0.09485325962305069
Batch 300, Loss: 0.06256510317325592
Epoch 55
-------------------------------
Batch 0, Loss: 0.048608921468257904
Batch 100, Loss: 0.13154932856559753
Batch 200, Loss: 0.20763058960437775
Batch 300, Loss: 0.3497653007507324
Epoch 56
-------------------------------
Batch 0, Loss: 0.2453620731830597
Batch 100, Loss: 0.09899668395519257
Batch 200, Loss: 0.10529981553554535
Batch 300, Loss: 0.18735143542289734
Epoch 57
-------------------------------
Batch 0, Loss: 0.1849079430103302
Batch 100, Loss: 0.061779383569955826
Batch 200, Loss: 0.1423257440328598
Batch 300, Loss: 0.12304048985242844
Epoch 58
-------------------------------
Batch 0, Loss: 0.07692559063434601
Batch 100, Loss: 0.054233845323324203
Batch 200, Loss: 0.16103693842887878
Batch 300, Loss: 0.1818847507238388
Early stopping
### Valid set performance:
Validation Accuracy: 0.7793721655328799
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.79      0.15      0.25       176
       BEFORE       0.90      0.85      0.88      2193
BEFOREOVERLAP       0.26      0.40      0.32        75
      OVERLAP       0.57      0.75      0.65       685

     accuracy                           0.78      3129
    macro avg       0.63      0.54      0.52      3129
 weighted avg       0.80      0.78      0.78      3129

### Summary
               precision    recall  f1-score   support

BEFOREOVERLAP       0.26      0.40      0.32        75
        AFTER       0.79      0.15      0.25       176
      OVERLAP       0.57      0.75      0.65       685
       BEFORE       0.90      0.85      0.88      2193

     accuracy                           0.78      3129
    macro avg       0.63      0.54      0.52      3129
 weighted avg       0.80      0.78      0.78      3129

### Test set performance:
Validation Accuracy: 0.7882288629737609
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.88      0.15      0.25       236
       BEFORE       0.91      0.86      0.88      2741
BEFOREOVERLAP       0.33      0.46      0.39        97
      OVERLAP       0.57      0.77      0.65       837

     accuracy                           0.79      3911
    macro avg       0.67      0.56      0.54      3911
 weighted avg       0.82      0.79      0.78      3911

### Summary
               precision    recall  f1-score   support

BEFOREOVERLAP       0.33      0.46      0.39        97
        AFTER       0.88      0.15      0.25       236
      OVERLAP       0.57      0.77      0.65       837
       BEFORE       0.91      0.86      0.88      2741

     accuracy                           0.79      3911
    macro avg       0.67      0.56      0.54      3911
 weighted avg       0.82      0.79      0.78      3911

Finished with this task.
Process finished!
