Spawning shell within /cluster/home/julievt/.cache/pypoetry/virtualenvs/master-project-118ajfdt-py3.12
Installing dependencies from lock file

No dependencies to install or update

Installing the current project: master-project (0.1.0)
##### Start training for TRE... ######
### Processing the files:
###### (0) Training for configuration file: c-bert-bilstm.ini
LOAD False
Using: cuda:0 with NN
{'BEFORE': 13827, 'OVERLAP': 4112, 'BEFOREOVERLAP': 491, 'AFTER': 1113}
{'BEFORE': 0.3533485210096189, 'OVERLAP': 1.188168774319066, 'BEFOREOVERLAP': 9.95061099796334, 'AFTER': 4.389712488769092}
Create sweep with ID: zg54gigm
Sweep URL: https://wandb.ai/julievt-ntnu/tre-dtr-c-Task.SEQUENCE-nn-model/sweeps/zg54gigm
{'batch_size': 32, 'early_stopping_delta': 0.01, 'early_stopping_patience': 3, 'embedding_dim': 32, 'epochs': 100, 'learning_rate': 0.0001, 'max_length': 64, 'num_workers': 0, 'optimizer': 'adam', 'shuffle': True, 'valid_batch_size': 16, 'weight_decay': 0}
Epoch 1
-------------------------------
Batch 0, Loss: 1.3295835256576538
Batch 100, Loss: 0.6974708437919617
Batch 200, Loss: 0.4439801871776581
Batch 300, Loss: 0.40556877851486206
Epoch 2
-------------------------------
Batch 0, Loss: 0.48885056376457214
Batch 100, Loss: 0.8662649989128113
Batch 200, Loss: 0.5253766775131226
Batch 300, Loss: 0.47692039608955383
Epoch 3
-------------------------------
Batch 0, Loss: 0.3208765387535095
Batch 100, Loss: 0.6385104656219482
Batch 200, Loss: 0.7728227376937866
Batch 300, Loss: 0.48201096057891846
Epoch 4
-------------------------------
Batch 0, Loss: 0.3746716380119324
Batch 100, Loss: 0.5154296159744263
Batch 200, Loss: 0.5803680419921875
Batch 300, Loss: 0.34850507974624634
Epoch 5
-------------------------------
Batch 0, Loss: 0.2951768636703491
Batch 100, Loss: 0.5455518960952759
Batch 200, Loss: 0.4122174382209778
Batch 300, Loss: 0.4190467298030853
Epoch 6
-------------------------------
Batch 0, Loss: 0.30119410157203674
Batch 100, Loss: 0.4972957968711853
Batch 200, Loss: 0.29072293639183044
Batch 300, Loss: 0.391810804605484
Epoch 7
-------------------------------
Batch 0, Loss: 0.1909673810005188
Batch 100, Loss: 0.18037447333335876
Batch 200, Loss: 0.4470103085041046
Batch 300, Loss: 0.1920875608921051
Epoch 8
-------------------------------
Batch 0, Loss: 0.34243419766426086
Batch 100, Loss: 0.45871415734291077
Batch 200, Loss: 0.13005606830120087
Batch 300, Loss: 0.2688137888908386
Epoch 9
-------------------------------
Batch 0, Loss: 0.3607727587223053
Batch 100, Loss: 0.39940914511680603
Batch 200, Loss: 0.20844222605228424
Batch 300, Loss: 0.24055713415145874
Epoch 10
-------------------------------
Batch 0, Loss: 0.1286022961139679
Batch 100, Loss: 0.29406824707984924
Batch 200, Loss: 0.19891436398029327
Batch 300, Loss: 0.32108062505722046
Early stopping
### Valid set performance:
Validation Accuracy: 0.7751838823153182
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       175
       BEFORE       0.94      0.83      0.88      2197
BEFOREOVERLAP       0.29      0.45      0.35        87
      OVERLAP       0.54      0.85      0.66       668

     accuracy                           0.78      3127
    macro avg       0.44      0.53      0.47      3127
 weighted avg       0.78      0.78      0.77      3127

### Summary
               precision    recall  f1-score   support

       BEFORE       0.94      0.83      0.88      2197
      OVERLAP       0.54      0.85      0.66       668
        AFTER       0.00      0.00      0.00       175
BEFOREOVERLAP       0.29      0.45      0.35        87

     accuracy                           0.78      3127
    macro avg       0.44      0.53      0.47      3127
 weighted avg       0.78      0.78      0.77      3127

### Test set performance:
Validation Accuracy: 0.7695062675876183
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       222
       BEFORE       0.94      0.82      0.88      2776
BEFOREOVERLAP       0.23      0.46      0.31        84
      OVERLAP       0.53      0.83      0.64       827

     accuracy                           0.77      3909
    macro avg       0.42      0.53      0.46      3909
 weighted avg       0.78      0.77      0.76      3909

### Summary
               precision    recall  f1-score   support

       BEFORE       0.94      0.82      0.88      2776
      OVERLAP       0.53      0.83      0.64       827
        AFTER       0.00      0.00      0.00       222
BEFOREOVERLAP       0.23      0.46      0.31        84

     accuracy                           0.77      3909
    macro avg       0.42      0.53      0.46      3909
 weighted avg       0.78      0.77      0.76      3909

{'batch_size': 32, 'early_stopping_delta': 0.01, 'early_stopping_patience': 3, 'embedding_dim': 32, 'epochs': 100, 'learning_rate': 0.0001, 'max_length': 64, 'num_workers': 0, 'optimizer': 'adam', 'shuffle': True, 'valid_batch_size': 16, 'weight_decay': 1e-05}
Epoch 1
-------------------------------
Batch 0, Loss: 1.3895992040634155
Batch 100, Loss: 0.5301825404167175
Batch 200, Loss: 0.736103355884552
Batch 300, Loss: 0.9153435230255127
Epoch 2
-------------------------------
Batch 0, Loss: 0.3628414273262024
Batch 100, Loss: 0.7066671252250671
Batch 200, Loss: 0.27296653389930725
Batch 300, Loss: 0.6559566259384155
Epoch 3
-------------------------------
Batch 0, Loss: 0.36589470505714417
Batch 100, Loss: 0.40191808342933655
Batch 200, Loss: 0.3554701805114746
Batch 300, Loss: 0.2519749104976654
Epoch 4
-------------------------------
Batch 0, Loss: 0.26800239086151123
Batch 100, Loss: 0.2953190207481384
Batch 200, Loss: 0.49520042538642883
Batch 300, Loss: 0.768375039100647
Epoch 5
-------------------------------
Batch 0, Loss: 0.19528041779994965
Batch 100, Loss: 0.13488224148750305
Batch 200, Loss: 0.1731480360031128
Batch 300, Loss: 0.6195483803749084
Epoch 6
-------------------------------
Batch 0, Loss: 0.16255855560302734
Batch 100, Loss: 0.1694377213716507
Batch 200, Loss: 0.45010247826576233
Batch 300, Loss: 0.19213812053203583
Epoch 7
-------------------------------
Batch 0, Loss: 0.22227229177951813
Batch 100, Loss: 0.29705148935317993
Batch 200, Loss: 0.428918719291687
Batch 300, Loss: 0.2027117758989334
Epoch 8
-------------------------------
Batch 0, Loss: 0.37113234400749207
Batch 100, Loss: 0.29022735357284546
Batch 200, Loss: 0.12472362816333771
Batch 300, Loss: 0.1379053145647049
Epoch 9
-------------------------------
Batch 0, Loss: 0.35645145177841187
Batch 100, Loss: 0.520362138748169
Batch 200, Loss: 0.10802286118268967
Batch 300, Loss: 0.11139381676912308
Epoch 10
-------------------------------
Batch 0, Loss: 0.0990818589925766
Batch 100, Loss: 0.10006483644247055
Batch 200, Loss: 0.15624259412288666
Batch 300, Loss: 0.3370826840400696
Epoch 11
-------------------------------
Batch 0, Loss: 0.06927375495433807
Batch 100, Loss: 0.10316658765077591
Batch 200, Loss: 0.17833057045936584
Batch 300, Loss: 0.043243102729320526
Epoch 12
-------------------------------
Batch 0, Loss: 0.19949272274971008
Batch 100, Loss: 0.04918380454182625
Batch 200, Loss: 0.12409576773643494
Batch 300, Loss: 0.05362922325730324
Epoch 13
-------------------------------
Batch 0, Loss: 0.5907301306724548
Batch 100, Loss: 0.14045026898384094
Batch 200, Loss: 0.18616782128810883
Batch 300, Loss: 0.03986648470163345
Early stopping
### Valid set performance:
Validation Accuracy: 0.8097217780620403
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.81      0.15      0.25       175
       BEFORE       0.90      0.90      0.90      2197
BEFOREOVERLAP       0.32      0.32      0.32        87
      OVERLAP       0.62      0.76      0.68       668

     accuracy                           0.81      3127
    macro avg       0.66      0.53      0.54      3127
 weighted avg       0.82      0.81      0.80      3127

### Summary
               precision    recall  f1-score   support

       BEFORE       0.90      0.90      0.90      2197
        AFTER       0.81      0.15      0.25       175
      OVERLAP       0.62      0.76      0.68       668
BEFOREOVERLAP       0.32      0.32      0.32        87

     accuracy                           0.81      3127
    macro avg       0.66      0.53      0.54      3127
 weighted avg       0.82      0.81      0.80      3127

### Test set performance:
Validation Accuracy: 0.8060885136863648
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.84      0.14      0.24       222
       BEFORE       0.91      0.89      0.90      2776
BEFOREOVERLAP       0.33      0.33      0.33        84
      OVERLAP       0.59      0.75      0.66       827

     accuracy                           0.81      3909
    macro avg       0.67      0.53      0.53      3909
 weighted avg       0.82      0.81      0.80      3909

### Summary
               precision    recall  f1-score   support

       BEFORE       0.91      0.89      0.90      2776
        AFTER       0.84      0.14      0.24       222
      OVERLAP       0.59      0.75      0.66       827
BEFOREOVERLAP       0.33      0.33      0.33        84

     accuracy                           0.81      3909
    macro avg       0.67      0.53      0.53      3909
 weighted avg       0.82      0.81      0.80      3909

{'batch_size': 32, 'early_stopping_delta': 0.01, 'early_stopping_patience': 3, 'embedding_dim': 32, 'epochs': 100, 'learning_rate': 0.0001, 'max_length': 64, 'num_workers': 0, 'optimizer': 'adam', 'shuffle': True, 'valid_batch_size': 16, 'weight_decay': 0.0001}
Epoch 1
-------------------------------
Batch 0, Loss: 1.4081780910491943
Batch 100, Loss: 0.5701035261154175
Batch 200, Loss: 0.40790143609046936
Batch 300, Loss: 0.7758392095565796
Epoch 2
-------------------------------
Batch 0, Loss: 0.8513971567153931
Batch 100, Loss: 0.4113418459892273
Batch 200, Loss: 0.37809139490127563
Batch 300, Loss: 0.5515689253807068
Epoch 3
-------------------------------
Batch 0, Loss: 0.5104546546936035
Batch 100, Loss: 0.40889865159988403
Batch 200, Loss: 0.5037500262260437
Batch 300, Loss: 0.24804699420928955
Epoch 4
-------------------------------
Batch 0, Loss: 0.27978864312171936
Batch 100, Loss: 0.1855236440896988
Batch 200, Loss: 0.25133708119392395
Batch 300, Loss: 0.21861249208450317
Epoch 5
-------------------------------
Batch 0, Loss: 0.5458776950836182
Batch 100, Loss: 0.39759647846221924
Batch 200, Loss: 0.39033567905426025
Batch 300, Loss: 0.3650049567222595
Epoch 6
-------------------------------
Batch 0, Loss: 0.5085797309875488
Batch 100, Loss: 0.17490477859973907
Batch 200, Loss: 0.418986052274704
Batch 300, Loss: 0.2830680310726166
Epoch 7
-------------------------------
Batch 0, Loss: 0.26847922801971436
Batch 100, Loss: 0.4087305963039398
Batch 200, Loss: 0.37465083599090576
Batch 300, Loss: 0.14995165169239044
Epoch 8
-------------------------------
Batch 0, Loss: 0.18926548957824707
Batch 100, Loss: 0.29341453313827515
Batch 200, Loss: 0.2908637821674347
Batch 300, Loss: 0.1443387120962143
Epoch 9
-------------------------------
Batch 0, Loss: 0.0996476486325264
Batch 100, Loss: 0.15096674859523773
Batch 200, Loss: 0.5638146996498108
Batch 300, Loss: 0.19448673725128174
Epoch 10
-------------------------------
Batch 0, Loss: 0.24667362868785858
Batch 100, Loss: 0.13029228150844574
Batch 200, Loss: 0.1158776804804802
Batch 300, Loss: 0.2925565540790558
Early stopping
### Valid set performance:
Validation Accuracy: 0.7847777422449632
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       175
       BEFORE       0.93      0.85      0.88      2197
BEFOREOVERLAP       0.42      0.13      0.19        87
      OVERLAP       0.53      0.87      0.66       668

     accuracy                           0.78      3127
    macro avg       0.47      0.46      0.44      3127
 weighted avg       0.78      0.78      0.77      3127

### Summary
               precision    recall  f1-score   support

       BEFORE       0.93      0.85      0.88      2197
      OVERLAP       0.53      0.87      0.66       668
        AFTER       0.00      0.00      0.00       175
BEFOREOVERLAP       0.42      0.13      0.19        87

     accuracy                           0.78      3127
    macro avg       0.47      0.46      0.44      3127
 weighted avg       0.78      0.78      0.77      3127

### Test set performance:
Validation Accuracy: 0.7784599641852136
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       1.00      0.00      0.01       222
       BEFORE       0.93      0.84      0.88      2776
BEFOREOVERLAP       0.34      0.17      0.22        84
      OVERLAP       0.52      0.85      0.64       827

     accuracy                           0.78      3909
    macro avg       0.70      0.46      0.44      3909
 weighted avg       0.83      0.78      0.77      3909

### Summary
               precision    recall  f1-score   support

       BEFORE       0.93      0.84      0.88      2776
        AFTER       1.00      0.00      0.01       222
      OVERLAP       0.52      0.85      0.64       827
BEFOREOVERLAP       0.34      0.17      0.22        84

     accuracy                           0.78      3909
    macro avg       0.70      0.46      0.44      3909
 weighted avg       0.83      0.78      0.77      3909

{'batch_size': 32, 'early_stopping_delta': 0.01, 'early_stopping_patience': 3, 'embedding_dim': 32, 'epochs': 100, 'learning_rate': 0.0001, 'max_length': 64, 'num_workers': 0, 'optimizer': 'adam', 'shuffle': True, 'valid_batch_size': 16, 'weight_decay': 0.001}
Epoch 1
-------------------------------
Batch 0, Loss: 1.400001883506775
Batch 100, Loss: 0.6856939196586609
Batch 200, Loss: 0.922842800617218
Batch 300, Loss: 0.6248627305030823
Epoch 2
-------------------------------
Batch 0, Loss: 0.5475394129753113
Batch 100, Loss: 0.3588082194328308
Batch 200, Loss: 0.6551290154457092
Batch 300, Loss: 0.6885921359062195
Epoch 3
-------------------------------
Batch 0, Loss: 0.4046899974346161
Batch 100, Loss: 0.6415786743164062
Batch 200, Loss: 0.46971195936203003
Batch 300, Loss: 0.8127194046974182
Epoch 4
-------------------------------
Batch 0, Loss: 0.5382744669914246
Batch 100, Loss: 0.41413190960884094
Batch 200, Loss: 0.6728570461273193
Batch 300, Loss: 0.49967724084854126
Epoch 5
-------------------------------
Batch 0, Loss: 0.4902544319629669
Batch 100, Loss: 0.31669604778289795
Batch 200, Loss: 0.24290965497493744
Batch 300, Loss: 0.4128866493701935
Epoch 6
-------------------------------
Batch 0, Loss: 0.17269515991210938
Batch 100, Loss: 0.4147862493991852
Batch 200, Loss: 0.3120320439338684
Batch 300, Loss: 0.3840688467025757
Epoch 7
-------------------------------
Batch 0, Loss: 0.1697520762681961
Batch 100, Loss: 0.5436570048332214
Batch 200, Loss: 0.13251259922981262
Batch 300, Loss: 0.37331220507621765
Epoch 8
-------------------------------
Batch 0, Loss: 0.5343376994132996
Batch 100, Loss: 0.14452926814556122
Batch 200, Loss: 0.26639702916145325
Batch 300, Loss: 0.13828951120376587
Epoch 9
-------------------------------
Batch 0, Loss: 0.3323565423488617
Batch 100, Loss: 0.29873189330101013
Batch 200, Loss: 0.20922958850860596
Batch 300, Loss: 0.23546157777309418
Epoch 10
-------------------------------
Batch 0, Loss: 0.1555110365152359
Batch 100, Loss: 0.13207009434700012
Batch 200, Loss: 0.19449397921562195
Batch 300, Loss: 0.3250395655632019
Early stopping
### Valid set performance:
Validation Accuracy: 0.7649504317236968
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       175
       BEFORE       0.91      0.83      0.87      2197
BEFOREOVERLAP       0.32      0.13      0.18        87
      OVERLAP       0.51      0.84      0.64       668

     accuracy                           0.76      3127
    macro avg       0.44      0.45      0.42      3127
 weighted avg       0.76      0.76      0.75      3127

### Summary
               precision    recall  f1-score   support

       BEFORE       0.91      0.83      0.87      2197
      OVERLAP       0.51      0.84      0.64       668
        AFTER       0.00      0.00      0.00       175
BEFOREOVERLAP       0.32      0.13      0.18        87

     accuracy                           0.76      3127
    macro avg       0.44      0.45      0.42      3127
 weighted avg       0.76      0.76      0.75      3127

### Test set performance:
Validation Accuracy: 0.7725761064210795
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       222
       BEFORE       0.92      0.84      0.87      2776
BEFOREOVERLAP       0.41      0.15      0.22        84
      OVERLAP       0.51      0.83      0.63       827

     accuracy                           0.77      3909
    macro avg       0.46      0.46      0.43      3909
 weighted avg       0.77      0.77      0.76      3909

### Summary
               precision    recall  f1-score   support

       BEFORE       0.92      0.84      0.87      2776
      OVERLAP       0.51      0.83      0.63       827
        AFTER       0.00      0.00      0.00       222
BEFOREOVERLAP       0.41      0.15      0.22        84

     accuracy                           0.77      3909
    macro avg       0.46      0.46      0.43      3909
 weighted avg       0.77      0.77      0.76      3909

{'batch_size': 32, 'early_stopping_delta': 0.01, 'early_stopping_patience': 3, 'embedding_dim': 32, 'epochs': 100, 'learning_rate': 0.0001, 'max_length': 64, 'num_workers': 0, 'optimizer': 'adam', 'shuffle': True, 'valid_batch_size': 16, 'weight_decay': 0.01}
Epoch 1
-------------------------------
Batch 0, Loss: 1.404117226600647
Batch 100, Loss: 0.44287213683128357
Batch 200, Loss: 0.7014195919036865
Batch 300, Loss: 0.6832678318023682
Epoch 2
-------------------------------
Batch 0, Loss: 0.6344648599624634
Batch 100, Loss: 0.5344382524490356
Batch 200, Loss: 0.3596329391002655
Batch 300, Loss: 0.40165483951568604
Epoch 3
-------------------------------
Batch 0, Loss: 0.5426591634750366
Batch 100, Loss: 0.5155035853385925
Batch 200, Loss: 0.448098361492157
Batch 300, Loss: 0.5341327786445618
Epoch 4
-------------------------------
Batch 0, Loss: 0.40990960597991943
Batch 100, Loss: 0.3405016362667084
Batch 200, Loss: 0.3807760179042816
Batch 300, Loss: 0.31645023822784424
Epoch 5
-------------------------------
Batch 0, Loss: 0.39525580406188965
Batch 100, Loss: 0.5472233295440674
Batch 200, Loss: 0.26872771978378296
Batch 300, Loss: 0.20114187896251678
Epoch 6
-------------------------------
Batch 0, Loss: 0.4588221609592438
Batch 100, Loss: 0.3209766149520874
Batch 200, Loss: 0.5889257788658142
Batch 300, Loss: 0.3818419277667999
Epoch 7
-------------------------------
Batch 0, Loss: 0.4377528429031372
Batch 100, Loss: 0.24464821815490723
Batch 200, Loss: 0.3342636823654175
Batch 300, Loss: 0.3276195526123047
Epoch 8
-------------------------------
Batch 0, Loss: 0.32864025235176086
Batch 100, Loss: 0.5518242120742798
Batch 200, Loss: 0.07219459116458893
Batch 300, Loss: 0.23695825040340424
Early stopping
### Valid set performance:
Validation Accuracy: 0.7659098177166613
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       175
       BEFORE       0.94      0.81      0.87      2197
BEFOREOVERLAP       0.32      0.32      0.32        87
      OVERLAP       0.51      0.88      0.65       668

     accuracy                           0.77      3127
    macro avg       0.44      0.50      0.46      3127
 weighted avg       0.78      0.77      0.76      3127

### Summary
               precision    recall  f1-score   support

       BEFORE       0.94      0.81      0.87      2197
      OVERLAP       0.51      0.88      0.65       668
        AFTER       0.00      0.00      0.00       175
BEFOREOVERLAP       0.32      0.32      0.32        87

     accuracy                           0.77      3127
    macro avg       0.44      0.50      0.46      3127
 weighted avg       0.78      0.77      0.76      3127

### Test set performance:
Validation Accuracy: 0.7656689690457917
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       222
       BEFORE       0.94      0.81      0.87      2776
BEFOREOVERLAP       0.32      0.32      0.32        84
      OVERLAP       0.50      0.87      0.64       827

     accuracy                           0.77      3909
    macro avg       0.44      0.50      0.46      3909
 weighted avg       0.78      0.77      0.76      3909

### Summary
               precision    recall  f1-score   support

       BEFORE       0.94      0.81      0.87      2776
      OVERLAP       0.50      0.87      0.64       827
        AFTER       0.00      0.00      0.00       222
BEFOREOVERLAP       0.32      0.32      0.32        84

     accuracy                           0.77      3909
    macro avg       0.44      0.50      0.46      3909
 weighted avg       0.78      0.77      0.76      3909

Finished with this task.
###### (2) Training for configuration file: b-bert.ini
LOAD False
Using: cuda:0 with BERT
Create sweep with ID: kbwju2bk
Sweep URL: https://wandb.ai/julievt-ntnu/tre-dtr-b-Task.SEQUENCE-bert-model/sweeps/kbwju2bk
{'BEFORE': 24490, 'OVERLAP': 6544, 'BEFOREOVERLAP': 804, 'AFTER': 1744}
{'BEFORE': 0.34281339322172316, 'OVERLAP': 1.2829309290953546, 'BEFOREOVERLAP': 10.442164179104477, 'AFTER': 4.813933486238532}
Training Epoch: 0
Training loss epoch: 0.5513144731521606
Training accuracy epoch: 0.6425316220238095
Overall acc: 0.6424716173459892
Overall loss: 370.48333740234375
Validation loss per 100 evaluation steps: 1.0739281177520752
Validation loss per 100 evaluation steps: 1.0339443952140241
Validation loss per 100 evaluation steps: 1.0067710492444868
Validation loss per 100 evaluation steps: 1.0120318278521794
Validation loss per 100 evaluation steps: 1.0196618639545845
Validation Loss: 1.0095278816918531
Validation Accuracy: 0.648489010989011
Overall accuracy: 0.6485037963376508
Overall loss: 1.0095278816918531
Training Epoch: 1
Training loss epoch: 0.44757604598999023
Training accuracy epoch: 0.7297247023809523
Overall acc: 0.7297133817234319
Overall loss: 300.7710876464844
Validation loss per 100 evaluation steps: 0.7043063044548035
Validation loss per 100 evaluation steps: 0.8705626534353389
Validation loss per 100 evaluation steps: 0.8439550406452435
Validation loss per 100 evaluation steps: 0.8419852321627133
Validation loss per 100 evaluation steps: 0.842939743042884
Validation Loss: 0.8411583958282357
Validation Accuracy: 0.6948031135531135
Overall accuracy: 0.694804228078011
Overall loss: 0.8411583958282357
Training Epoch: 2
Training loss epoch: 0.3686057925224304
Training accuracy epoch: 0.7787388392857143
Overall acc: 0.7787548855388051
Overall loss: 247.7030792236328
Validation loss per 100 evaluation steps: 1.304556965827942
Validation loss per 100 evaluation steps: 0.8572437971609064
Validation loss per 100 evaluation steps: 0.8815606470168823
Validation loss per 100 evaluation steps: 0.8919056962519587
Validation loss per 100 evaluation steps: 0.8600492222220671
Validation Loss: 0.8627561292211924
Validation Accuracy: 0.7028044871794873
Overall accuracy: 0.7028435313383952
Overall loss: 0.8627561292211924
Training Epoch: 3
Training loss epoch: 0.3000873923301697
Training accuracy epoch: 0.8145275297619047
Overall acc: 0.814535641168807
Overall loss: 201.65872192382812
Validation loss per 100 evaluation steps: 0.7527386546134949
Validation loss per 100 evaluation steps: 0.7230419414025722
Validation loss per 100 evaluation steps: 0.685315750353965
Validation loss per 100 evaluation steps: 0.704725597536445
Validation loss per 100 evaluation steps: 0.7130170894456921
Validation Loss: 0.7173031125395071
Validation Accuracy: 0.7632898351648352
Overall accuracy: 0.7632871817775793
Overall loss: 0.7173031125395071
Training Epoch: 4
Training loss epoch: 0.247717946767807
Training accuracy epoch: 0.8397321428571428
Overall acc: 0.8397543271915131
Overall loss: 166.46646118164062
Validation loss per 100 evaluation steps: 0.4019319713115692
Validation loss per 100 evaluation steps: 0.7341039273703452
Validation loss per 100 evaluation steps: 0.736211314834469
Validation loss per 100 evaluation steps: 0.7332772958120238
Validation loss per 100 evaluation steps: 0.719899171345549
Validation Loss: 0.7188962821094763
Validation Accuracy: 0.7572687728937729
Overall accuracy: 0.7573321423254429
Overall loss: 0.7188962821094763
Training Epoch: 5
Training loss epoch: 0.19757547974586487
Training accuracy epoch: 0.86640625
Overall acc: 0.8664154103852596
Overall loss: 132.77072143554688
Validation loss per 100 evaluation steps: 0.20899739861488342
Validation loss per 100 evaluation steps: 0.6284093997708642
Validation loss per 100 evaluation steps: 0.6501663872295648
Validation loss per 100 evaluation steps: 0.6565033753968751
Validation loss per 100 evaluation steps: 0.6597949769301457
Validation Loss: 0.6558083023787254
Validation Accuracy: 0.7846382783882784
Overall accuracy: 0.7845764478189668
Overall loss: 0.6558083023787254
Training Epoch: 6
Training loss epoch: 0.1718282252550125
Training accuracy epoch: 0.8852120535714285
Overall acc: 0.8852596314907872
Overall loss: 115.46856689453125
Validation loss per 100 evaluation steps: 0.6613534688949585
Validation loss per 100 evaluation steps: 0.7793087785012356
Validation loss per 100 evaluation steps: 0.7718528515719507
Validation loss per 100 evaluation steps: 0.7836243567796443
Validation loss per 100 evaluation steps: 0.7861049697267593
Validation Loss: 0.7888778105466848
Validation Accuracy: 0.7607600732600732
Overall accuracy: 0.7607562900104213
Overall loss: 0.7888778105466848
Training Epoch: 7
Training loss epoch: 0.12665104866027832
Training accuracy epoch: 0.9109561011904762
Overall acc: 0.9109901358645077
Overall loss: 85.10950469970703
Validation loss per 100 evaluation steps: 0.25546810030937195
Validation loss per 100 evaluation steps: 0.794547409630648
Validation loss per 100 evaluation steps: 0.7728283103712978
Validation loss per 100 evaluation steps: 0.7568662948875481
Validation loss per 100 evaluation steps: 0.7649113799589327
Validation Loss: 0.7658710304880515
Validation Accuracy: 0.8069253663003664
Overall accuracy: 0.8069078457644782
Overall loss: 0.7658710304880515
Training Epoch: 8
Training loss epoch: 0.10842827707529068
Training accuracy epoch: 0.9266555059523809
Overall acc: 0.9266703889819468
Overall loss: 72.86380004882812
Validation loss per 100 evaluation steps: 1.4495799541473389
Validation loss per 100 evaluation steps: 0.9347441537849752
Validation loss per 100 evaluation steps: 0.9616215762830529
Validation loss per 100 evaluation steps: 0.9704172083028156
Validation loss per 100 evaluation steps: 0.978323080213485
Validation Loss: 0.9746984584378966
Validation Accuracy: 0.759569597069597
Overall accuracy: 0.7595652821199941
Overall loss: 0.9746984584378966
Early stopping
### Valid set performance:
Validation loss per 100 evaluation steps: 1.099705696105957
Validation loss per 100 evaluation steps: 0.925900255339128
Validation loss per 100 evaluation steps: 0.9272815624891377
Validation loss per 100 evaluation steps: 0.9297375990715236
Validation Loss: 0.9365635781072169
Validation Accuracy: 0.7660971840659341
Overall accuracy: 0.7660524846454495
Overall loss: 0.9365635781072169
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.87      0.05      0.09       284
       BEFORE       0.96      0.80      0.87      3870
BEFOREOVERLAP       0.45      0.25      0.32       140
      OVERLAP       0.48      0.91      0.63      1079

     accuracy                           0.77      5373
    macro avg       0.69      0.50      0.48      5373
 weighted avg       0.84      0.77      0.77      5373

### Summary
               precision    recall  f1-score   support

       BEFORE       0.96      0.80      0.87      3870
        AFTER       0.87      0.05      0.09       284
      OVERLAP       0.48      0.91      0.63      1079
BEFOREOVERLAP       0.45      0.25      0.32       140

     accuracy                           0.77      5373
    macro avg       0.69      0.50      0.48      5373
 weighted avg       0.84      0.77      0.77      5373

### Test set performance:
Validation loss per 100 evaluation steps: 1.0355017185211182
Validation loss per 100 evaluation steps: 0.9555355275031363
Validation loss per 100 evaluation steps: 0.9690335597958768
Validation loss per 100 evaluation steps: 0.953796757071084
Validation loss per 100 evaluation steps: 0.9812961027520869
Validation Loss: 0.9744849201480281
Validation Accuracy: 0.7596039377289378
Overall accuracy: 0.7595652821199941
Overall loss: 0.9744849201480281
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.89      0.05      0.09       363
       BEFORE       0.95      0.79      0.86      4877
BEFOREOVERLAP       0.45      0.24      0.31       160
      OVERLAP       0.46      0.89      0.61      1317

     accuracy                           0.76      6717
    macro avg       0.69      0.49      0.47      6717
 weighted avg       0.84      0.76      0.76      6717

### Summary
               precision    recall  f1-score   support

       BEFORE       0.95      0.79      0.86      4877
        AFTER       0.89      0.05      0.09       363
      OVERLAP       0.46      0.89      0.61      1317
BEFOREOVERLAP       0.45      0.24      0.31       160

     accuracy                           0.76      6717
    macro avg       0.69      0.49      0.47      6717
 weighted avg       0.84      0.76      0.76      6717

{'BEFORE': 24490, 'OVERLAP': 6544, 'BEFOREOVERLAP': 804, 'AFTER': 1744}
{'BEFORE': 0.34281339322172316, 'OVERLAP': 1.2829309290953546, 'BEFOREOVERLAP': 10.442164179104477, 'AFTER': 4.813933486238532}
Training Epoch: 0
Training loss epoch: 0.5546254515647888
Training accuracy epoch: 0.6439546130952382
Overall acc: 0.6438674855760282
Overall loss: 372.7082824707031
Validation loss per 100 evaluation steps: 0.3478315770626068
Validation loss per 100 evaluation steps: 0.8560576255958859
Validation loss per 100 evaluation steps: 0.8608676368620858
Validation loss per 100 evaluation steps: 0.8534556075782079
Validation loss per 100 evaluation steps: 0.8683571233936676
Validation Loss: 0.8656300983613445
Validation Accuracy: 0.6580471611721611
Overall accuracy: 0.6580318594610689
Overall loss: 0.8656300983613445
Training Epoch: 1
Training loss epoch: 0.46184277534484863
Training accuracy epoch: 0.7258928571428571
Overall acc: 0.7258514796203238
Overall loss: 310.35833740234375
Validation loss per 100 evaluation steps: 0.5446155071258545
Validation loss per 100 evaluation steps: 0.8027224157116201
Validation loss per 100 evaluation steps: 0.7955509630305257
Validation loss per 100 evaluation steps: 0.8060289991159376
Validation loss per 100 evaluation steps: 0.8012497986381191
Validation Loss: 0.8015368752713714
Validation Accuracy: 0.7000457875457875
Overall accuracy: 0.7000148875986303
Overall loss: 0.8015368752713714
Training Epoch: 2
Training loss epoch: 0.3909216821193695
Training accuracy epoch: 0.7680431547619048
Overall acc: 0.7680532291085055
Overall loss: 262.6993713378906
Validation loss per 100 evaluation steps: 0.8129453659057617
Validation loss per 100 evaluation steps: 0.6943472293343874
Validation loss per 100 evaluation steps: 0.6961670218415521
Validation loss per 100 evaluation steps: 0.6853534467889622
Validation loss per 100 evaluation steps: 0.6738224079056719
Validation Loss: 0.6781461384413497
Validation Accuracy: 0.7645260989010989
Overall accuracy: 0.7646270656543099
Overall loss: 0.6781461384413497
Training Epoch: 3
Training loss epoch: 0.324868768453598
Training accuracy epoch: 0.8045479910714286
Overall acc: 0.8045784477945283
Overall loss: 218.3118133544922
Validation loss per 100 evaluation steps: 0.5504988431930542
Validation loss per 100 evaluation steps: 0.6172492248882161
Validation loss per 100 evaluation steps: 0.6478641002406529
Validation loss per 100 evaluation steps: 0.6700562894393836
Validation loss per 100 evaluation steps: 0.6656486092511555
Validation Loss: 0.6625711498515946
Validation Accuracy: 0.7810668498168498
Overall accuracy: 0.781003424147685
Overall loss: 0.6625711498515946
Training Epoch: 4
Training loss epoch: 0.2696605920791626
Training accuracy epoch: 0.8326171874999999
Overall acc: 0.8326353992183138
Overall loss: 181.2119140625
Validation loss per 100 evaluation steps: 1.0440773963928223
Validation loss per 100 evaluation steps: 0.6704311156922048
Validation loss per 100 evaluation steps: 0.6757227455205586
Validation loss per 100 evaluation steps: 0.6868905331258758
Validation loss per 100 evaluation steps: 0.6882117626041249
Validation Loss: 0.6898206516390755
Validation Accuracy: 0.7812156593406593
Overall accuracy: 0.7811523001339884
Overall loss: 0.6898206516390755
Training Epoch: 5
Training loss epoch: 0.21251100301742554
Training accuracy epoch: 0.8595424107142856
Overall acc: 0.8595756560580681
Overall loss: 142.80738830566406
Validation loss per 100 evaluation steps: 0.6708106994628906
Validation loss per 100 evaluation steps: 0.7729413791347554
Validation loss per 100 evaluation steps: 0.7375621566298738
Validation loss per 100 evaluation steps: 0.7384323137456495
Validation loss per 100 evaluation steps: 0.7209655600936707
Validation Loss: 0.7135275779158942
Validation Accuracy: 0.7890682234432235
Overall accuracy: 0.7890427274080691
Overall loss: 0.7135275779158942
Training Epoch: 6
Training loss epoch: 0.1779090166091919
Training accuracy epoch: 0.8821986607142857
Overall acc: 0.8821887213847013
Overall loss: 119.55485534667969
Validation loss per 100 evaluation steps: 0.7241991758346558
Validation loss per 100 evaluation steps: 0.6901713682357038
Validation loss per 100 evaluation steps: 0.7040211019008907
Validation loss per 100 evaluation steps: 0.7337152501351611
Validation loss per 100 evaluation steps: 0.7488168821091813
Validation Loss: 0.7567589377097431
Validation Accuracy: 0.7960622710622711
Overall accuracy: 0.7960398987643293
Overall loss: 0.7567589377097431
Early stopping
### Valid set performance:
Validation loss per 100 evaluation steps: 0.35905489325523376
Validation loss per 100 evaluation steps: 0.6512435510533281
Validation loss per 100 evaluation steps: 0.6990092112613258
Validation loss per 100 evaluation steps: 0.712973896983355
Validation Loss: 0.7180548848340377
Validation Accuracy: 0.807720924908425
Overall accuracy: 0.8077424157826167
Overall loss: 0.7180548848340377
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.92      0.17      0.29       284
       BEFORE       0.93      0.86      0.90      3870
BEFOREOVERLAP       0.48      0.31      0.38       140
      OVERLAP       0.55      0.84      0.67      1079

     accuracy                           0.81      5373
    macro avg       0.72      0.55      0.56      5373
 weighted avg       0.84      0.81      0.80      5373

### Summary
               precision    recall  f1-score   support

       BEFORE       0.93      0.86      0.90      3870
        AFTER       0.92      0.17      0.29       284
      OVERLAP       0.55      0.84      0.67      1079
BEFOREOVERLAP       0.48      0.31      0.38       140

     accuracy                           0.81      5373
    macro avg       0.72      0.55      0.56      5373
 weighted avg       0.84      0.81      0.80      5373

### Test set performance:
Validation loss per 100 evaluation steps: 0.7114179730415344
Validation loss per 100 evaluation steps: 0.7575275619136225
Validation loss per 100 evaluation steps: 0.7653775308373852
Validation loss per 100 evaluation steps: 0.7717628493731798
Validation loss per 100 evaluation steps: 0.7614304024187333
Validation Loss: 0.7567365907443067
Validation Accuracy: 0.7960622710622711
Overall accuracy: 0.7960398987643293
Overall loss: 0.7567365907443067
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.93      0.15      0.26       363
       BEFORE       0.92      0.86      0.89      4877
BEFOREOVERLAP       0.46      0.30      0.36       160
      OVERLAP       0.53      0.80      0.64      1317

     accuracy                           0.80      6717
    macro avg       0.71      0.53      0.54      6717
 weighted avg       0.83      0.80      0.79      6717

### Summary
               precision    recall  f1-score   support

       BEFORE       0.92      0.86      0.89      4877
        AFTER       0.93      0.15      0.26       363
      OVERLAP       0.53      0.80      0.64      1317
BEFOREOVERLAP       0.46      0.30      0.36       160

     accuracy                           0.80      6717
    macro avg       0.71      0.53      0.54      6717
 weighted avg       0.83      0.80      0.79      6717

{'BEFORE': 24490, 'OVERLAP': 6544, 'BEFOREOVERLAP': 804, 'AFTER': 1744}
{'BEFORE': 0.34281339322172316, 'OVERLAP': 1.2829309290953546, 'BEFOREOVERLAP': 10.442164179104477, 'AFTER': 4.813933486238532}
Training Epoch: 0
Training loss epoch: 0.5563541054725647
Training accuracy epoch: 0.6455357142857143
Overall acc: 0.6454494695700725
Overall loss: 373.86993408203125
Validation loss per 100 evaluation steps: 0.567456841468811
Validation loss per 100 evaluation steps: 0.8456195564848361
Validation loss per 100 evaluation steps: 0.8483390733970338
Validation loss per 100 evaluation steps: 0.8310739052454102
Validation loss per 100 evaluation steps: 0.8296034538240504
Validation Loss: 0.8332097548104468
Validation Accuracy: 0.6935325091575092
Overall accuracy: 0.6934643442012803
Overall loss: 0.8332097548104468
Training Epoch: 1
Training loss epoch: 0.45710068941116333
Training accuracy epoch: 0.7280691964285714
Overall acc: 0.7280848687883864
Overall loss: 307.1716613769531
Validation loss per 100 evaluation steps: 1.146343469619751
Validation loss per 100 evaluation steps: 0.8141583040504172
Validation loss per 100 evaluation steps: 0.8227743208408356
Validation loss per 100 evaluation steps: 0.8129408666461805
Validation loss per 100 evaluation steps: 0.8176521334676374
Validation Loss: 0.8137004911012593
Validation Accuracy: 0.7000801282051282
Overall accuracy: 0.7000148875986303
Overall loss: 0.8137004911012593
Training Epoch: 2
Training loss epoch: 0.3834135830402374
Training accuracy epoch: 0.7732607886904762
Overall acc: 0.7732179415596501
Overall loss: 257.6539306640625
Validation loss per 100 evaluation steps: 0.5666018724441528
Validation loss per 100 evaluation steps: 0.8718340333440516
Validation loss per 100 evaluation steps: 0.8663535741579473
Validation loss per 100 evaluation steps: 0.8582600201433679
Validation loss per 100 evaluation steps: 0.8547041206028396
Validation Loss: 0.8591085827244179
Validation Accuracy: 0.7011332417582418
Overall accuracy: 0.7012058954890577
Overall loss: 0.8591085827244179
Training Epoch: 3
Training loss epoch: 0.31145456433296204
Training accuracy epoch: 0.8070777529761906
Overall acc: 0.8069979527265959
Overall loss: 209.29745483398438
Validation loss per 100 evaluation steps: 0.6594455242156982
Validation loss per 100 evaluation steps: 0.7735516424521361
Validation loss per 100 evaluation steps: 0.7446231636686704
Validation loss per 100 evaluation steps: 0.7468598560736425
Validation loss per 100 evaluation steps: 0.7593391344062705
Validation Loss: 0.7605327516794205
Validation Accuracy: 0.747470238095238
Overall accuracy: 0.7473574512431145
Overall loss: 0.7605327516794205
Training Epoch: 4
Training loss epoch: 0.2510582208633423
Training accuracy epoch: 0.8394159226190476
Overall acc: 0.8393820956635027
Overall loss: 168.71112060546875
Validation loss per 100 evaluation steps: 0.6142300963401794
Validation loss per 100 evaluation steps: 0.6880886328065454
Validation loss per 100 evaluation steps: 0.6989148298917867
Validation loss per 100 evaluation steps: 0.682813547619702
Validation loss per 100 evaluation steps: 0.6710913353913145
Validation Loss: 0.6780754965153478
Validation Accuracy: 0.7952495421245421
Overall accuracy: 0.7952955188328122
Overall loss: 0.6780754965153478
Training Epoch: 5
Training loss epoch: 0.20216524600982666
Training accuracy epoch: 0.8681733630952382
Overall acc: 0.8681835101433092
Overall loss: 135.85504150390625
Validation loss per 100 evaluation steps: 0.839688777923584
Validation loss per 100 evaluation steps: 0.7042333485909028
Validation loss per 100 evaluation steps: 0.7027444993307934
Validation loss per 100 evaluation steps: 0.6898538959877831
Validation loss per 100 evaluation steps: 0.699789508954248
Validation Loss: 0.698024624834458
Validation Accuracy: 0.7920100732600732
Overall accuracy: 0.7920202471341372
Overall loss: 0.698024624834458
Training Epoch: 6
Training loss epoch: 0.16134649515151978
Training accuracy epoch: 0.8885602678571428
Overall acc: 0.8886097152428811
Overall loss: 108.42484283447266
Validation loss per 100 evaluation steps: 0.30737537145614624
Validation loss per 100 evaluation steps: 0.6491792488031753
Validation loss per 100 evaluation steps: 0.6834941084093568
Validation loss per 100 evaluation steps: 0.6978499598714502
Validation loss per 100 evaluation steps: 0.7059219240854283
Validation Loss: 0.7114860665097478
Validation Accuracy: 0.7961080586080587
Overall accuracy: 0.7961887747506328
Overall loss: 0.7114860665097478
Training Epoch: 7
Training loss epoch: 0.12919433414936066
Training accuracy epoch: 0.9092912946428572
Overall acc: 0.9092685650474596
Overall loss: 86.81858825683594
Validation loss per 100 evaluation steps: 0.7175000309944153
Validation loss per 100 evaluation steps: 0.757010281703776
Validation loss per 100 evaluation steps: 0.7544309920330176
Validation loss per 100 evaluation steps: 0.752420208543737
Validation loss per 100 evaluation steps: 0.7666502789271405
Validation Loss: 0.7633404892447981
Validation Accuracy: 0.8038003663003663
Overall accuracy: 0.8037814500521065
Overall loss: 0.7633404892447981
Early stopping
### Valid set performance:
Validation loss per 100 evaluation steps: 0.5854673981666565
Validation loss per 100 evaluation steps: 0.7187582107242381
Validation loss per 100 evaluation steps: 0.7262913592898783
Validation loss per 100 evaluation steps: 0.7294372292660932
Validation Loss: 0.7498172339061663
Validation Accuracy: 0.816148695054945
Overall accuracy: 0.8163037409268565
Overall loss: 0.7498172339061663
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.73      0.22      0.34       284
       BEFORE       0.92      0.89      0.90      3870
BEFOREOVERLAP       0.41      0.27      0.33       140
      OVERLAP       0.58      0.79      0.67      1079

     accuracy                           0.82      5373
    macro avg       0.66      0.54      0.56      5373
 weighted avg       0.83      0.82      0.81      5373

### Summary
               precision    recall  f1-score   support

       BEFORE       0.92      0.89      0.90      3870
        AFTER       0.73      0.22      0.34       284
      OVERLAP       0.58      0.79      0.67      1079
BEFOREOVERLAP       0.41      0.27      0.33       140

     accuracy                           0.82      5373
    macro avg       0.66      0.54      0.56      5373
 weighted avg       0.83      0.82      0.81      5373

### Test set performance:
Validation loss per 100 evaluation steps: 0.5581734776496887
Validation loss per 100 evaluation steps: 0.7198466640795664
Validation loss per 100 evaluation steps: 0.7615357176423888
Validation loss per 100 evaluation steps: 0.7765846089908599
Validation loss per 100 evaluation steps: 0.7632508240965313
Validation Loss: 0.763389952041741
Validation Accuracy: 0.8038347069597069
Overall accuracy: 0.8037814500521065
Overall loss: 0.763389952041741
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.70      0.25      0.37       363
       BEFORE       0.91      0.88      0.89      4877
BEFOREOVERLAP       0.31      0.24      0.27       160
      OVERLAP       0.56      0.75      0.64      1317

     accuracy                           0.80      6717
    macro avg       0.62      0.53      0.54      6717
 weighted avg       0.82      0.80      0.80      6717

### Summary
               precision    recall  f1-score   support

       BEFORE       0.91      0.88      0.89      4877
        AFTER       0.70      0.25      0.37       363
      OVERLAP       0.56      0.75      0.64      1317
BEFOREOVERLAP       0.31      0.24      0.27       160

     accuracy                           0.80      6717
    macro avg       0.62      0.53      0.54      6717
 weighted avg       0.82      0.80      0.80      6717

{'BEFORE': 24490, 'OVERLAP': 6544, 'BEFOREOVERLAP': 804, 'AFTER': 1744}
{'BEFORE': 0.34281339322172316, 'OVERLAP': 1.2829309290953546, 'BEFOREOVERLAP': 10.442164179104477, 'AFTER': 4.813933486238532}
Training Epoch: 0
Training loss epoch: 0.5693667531013489
Training accuracy epoch: 0.6319010416666666
Overall acc: 0.6318630187976921
Overall loss: 382.61444091796875
Validation loss per 100 evaluation steps: 1.0672175884246826
Validation loss per 100 evaluation steps: 0.9899921688703027
Validation loss per 100 evaluation steps: 0.9511948261984545
Validation loss per 100 evaluation steps: 0.9255784682657235
Validation loss per 100 evaluation steps: 0.9210318311714472
Validation Loss: 0.9206688339156764
Validation Accuracy: 0.6361378205128205
Overall accuracy: 0.6361470894744677
Overall loss: 0.9206688339156764
Training Epoch: 1
Training loss epoch: 0.4749554693698883
Training accuracy epoch: 0.7001395089285714
Overall acc: 0.7001675041876047
Overall loss: 319.1700744628906
Validation loss per 100 evaluation steps: 0.6838879585266113
Validation loss per 100 evaluation steps: 0.7243595050998254
Validation loss per 100 evaluation steps: 0.723306405529454
Validation loss per 100 evaluation steps: 0.7222498080560139
Validation loss per 100 evaluation steps: 0.7143997395796668
Validation Loss: 0.7233945149396147
Validation Accuracy: 0.7146405677655677
Overall accuracy: 0.7147536102426678
Overall loss: 0.7233945149396147
Training Epoch: 2
Training loss epoch: 0.4024972915649414
Training accuracy epoch: 0.7565104166666666
Overall acc: 0.7565140517401824
Overall loss: 270.4781799316406
Validation loss per 100 evaluation steps: 1.1279515027999878
Validation loss per 100 evaluation steps: 0.7403287205218089
Validation loss per 100 evaluation steps: 0.7205785918072681
Validation loss per 100 evaluation steps: 0.701614075745657
Validation loss per 100 evaluation steps: 0.7047236755750423
Validation Loss: 0.7073701541693438
Validation Accuracy: 0.7544070512820513
Overall accuracy: 0.7545034985856781
Overall loss: 0.7073701541693438
Training Epoch: 3
Training loss epoch: 0.331918329000473
Training accuracy epoch: 0.7995535714285713
Overall acc: 0.7995533221663875
Overall loss: 223.0491180419922
Validation loss per 100 evaluation steps: 1.0305137634277344
Validation loss per 100 evaluation steps: 0.7447941396201011
Validation loss per 100 evaluation steps: 0.7364538687082073
Validation loss per 100 evaluation steps: 0.7425098013986781
Validation loss per 100 evaluation steps: 0.7464100766248833
Validation Loss: 0.7484071263422568
Validation Accuracy: 0.7330815018315019
Overall accuracy: 0.7330653565579872
Overall loss: 0.7484071263422568
Training Epoch: 4
Training loss epoch: 0.2606602609157562
Training accuracy epoch: 0.8369977678571429
Overall acc: 0.836962590731435
Overall loss: 175.1636962890625
Validation loss per 100 evaluation steps: 0.26993343234062195
Validation loss per 100 evaluation steps: 0.7102100974557424
Validation loss per 100 evaluation steps: 0.7177202459517403
Validation loss per 100 evaluation steps: 0.739113377500016
Validation loss per 100 evaluation steps: 0.73295346659578
Validation Loss: 0.7444700516316862
Validation Accuracy: 0.757749542124542
Overall accuracy: 0.7577787702843531
Overall loss: 0.7444700516316862
Training Epoch: 5
Training loss epoch: 0.20562876760959625
Training accuracy epoch: 0.8665178571428571
Overall acc: 0.8665549972082636
Overall loss: 138.18252563476562
Validation loss per 100 evaluation steps: 0.3360534906387329
Validation loss per 100 evaluation steps: 0.6901428537203533
Validation loss per 100 evaluation steps: 0.669296735665988
Validation loss per 100 evaluation steps: 0.6806985800250425
Validation loss per 100 evaluation steps: 0.693726459982575
Validation Loss: 0.6932151612470903
Validation Accuracy: 0.7846840659340659
Overall accuracy: 0.7847253238052702
Overall loss: 0.6932151612470903
Training Epoch: 6
Training loss epoch: 0.15049804747104645
Training accuracy epoch: 0.9012276785714286
Overall acc: 0.9013121161362367
Overall loss: 101.13468933105469
Validation loss per 100 evaluation steps: 0.44299834966659546
Validation loss per 100 evaluation steps: 0.7723809084901125
Validation loss per 100 evaluation steps: 0.8025416007626858
Validation loss per 100 evaluation steps: 0.7755285378397007
Validation loss per 100 evaluation steps: 0.7747820920341107
Validation Loss: 0.7715850247380635
Validation Accuracy: 0.7724358974358974
Overall accuracy: 0.7723686169420872
Overall loss: 0.7715850247380635
Training Epoch: 7
Training loss epoch: 0.12070607393980026
Training accuracy epoch: 0.9154203869047619
Overall acc: 0.9154569142006328
Overall loss: 81.1144790649414
Validation loss per 100 evaluation steps: 0.5393062233924866
Validation loss per 100 evaluation steps: 0.9221363039963907
Validation loss per 100 evaluation steps: 0.8793851748590742
Validation loss per 100 evaluation steps: 0.8785687713664136
Validation loss per 100 evaluation steps: 0.8599543533708315
Validation Loss: 0.8644983125362723
Validation Accuracy: 0.7720695970695971
Overall accuracy: 0.7720708649694804
Overall loss: 0.8644983125362723
Training Epoch: 8
Training loss epoch: 0.10829004645347595
Training accuracy epoch: 0.9265904017857144
Overall acc: 0.9265773310999441
Overall loss: 72.77091217041016
Validation loss per 100 evaluation steps: 0.31011244654655457
Validation loss per 100 evaluation steps: 0.7986990301547074
Validation loss per 100 evaluation steps: 0.7873876885399779
Validation loss per 100 evaluation steps: 0.7688724353160722
Validation loss per 100 evaluation steps: 0.784247519975607
Validation Loss: 0.780635855810362
Validation Accuracy: 0.8074061355311355
Overall accuracy: 0.8073544737233884
Overall loss: 0.780635855810362
Early stopping
### Valid set performance:
Validation loss per 100 evaluation steps: 0.1135987639427185
Validation loss per 100 evaluation steps: 0.7741243132934121
Validation loss per 100 evaluation steps: 0.7335355129760148
Validation loss per 100 evaluation steps: 0.7643983336939071
Validation Loss: 0.7525000589140247
Validation Accuracy: 0.8144602793040294
Overall accuracy: 0.8144425832868044
Overall loss: 0.7525000589140247
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.71      0.23      0.35       284
       BEFORE       0.90      0.90      0.90      3870
BEFOREOVERLAP       0.46      0.31      0.37       140
      OVERLAP       0.59      0.72      0.65      1079

     accuracy                           0.81      5373
    macro avg       0.67      0.54      0.57      5373
 weighted avg       0.82      0.81      0.81      5373

### Summary
               precision    recall  f1-score   support

       BEFORE       0.90      0.90      0.90      3870
        AFTER       0.71      0.23      0.35       284
      OVERLAP       0.59      0.72      0.65      1079
BEFOREOVERLAP       0.46      0.31      0.37       140

     accuracy                           0.81      5373
    macro avg       0.67      0.54      0.57      5373
 weighted avg       0.82      0.81      0.81      5373

### Test set performance:
Validation loss per 100 evaluation steps: 0.009629854001104832
Validation loss per 100 evaluation steps: 0.7123585681679963
Validation loss per 100 evaluation steps: 0.7630700387190379
Validation loss per 100 evaluation steps: 0.7698896637378242
Validation loss per 100 evaluation steps: 0.7827268604542178
Validation Loss: 0.7809613798833674
Validation Accuracy: 0.8073717948717949
Overall accuracy: 0.8073544737233884
Overall loss: 0.7809613798833674
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.72      0.26      0.38       363
       BEFORE       0.90      0.89      0.90      4877
BEFOREOVERLAP       0.41      0.23      0.29       160
      OVERLAP       0.57      0.72      0.63      1317

     accuracy                           0.81      6717
    macro avg       0.65      0.52      0.55      6717
 weighted avg       0.81      0.81      0.80      6717

### Summary
               precision    recall  f1-score   support

       BEFORE       0.90      0.89      0.90      4877
        AFTER       0.72      0.26      0.38       363
      OVERLAP       0.57      0.72      0.63      1317
BEFOREOVERLAP       0.41      0.23      0.29       160

     accuracy                           0.81      6717
    macro avg       0.65      0.52      0.55      6717
 weighted avg       0.81      0.81      0.80      6717

{'BEFORE': 24490, 'OVERLAP': 6544, 'BEFOREOVERLAP': 804, 'AFTER': 1744}
{'BEFORE': 0.34281339322172316, 'OVERLAP': 1.2829309290953546, 'BEFOREOVERLAP': 10.442164179104477, 'AFTER': 4.813933486238532}
Training Epoch: 0
Training loss epoch: 0.5756694078445435
Training accuracy epoch: 0.6218656994047619
Overall acc: 0.6219058254234133
Overall loss: 386.8498229980469
Validation loss per 100 evaluation steps: 1.0530565977096558
Validation loss per 100 evaluation steps: 0.9257026842915186
Validation loss per 100 evaluation steps: 0.9687327511274992
Validation loss per 100 evaluation steps: 0.9726516661849924
Validation loss per 100 evaluation steps: 0.9719322796474371
Validation Loss: 0.9729450496889296
Validation Accuracy: 0.6053685897435898
Overall accuracy: 0.605329760309662
Overall loss: 0.9729450496889296
Training Epoch: 1
Training loss epoch: 0.5270101428031921
Training accuracy epoch: 0.6613653273809523
Overall acc: 0.6613158384515169
Overall loss: 354.15081787109375
Validation loss per 100 evaluation steps: 0.6233854293823242
Validation loss per 100 evaluation steps: 0.8415206373475566
Validation loss per 100 evaluation steps: 0.8354097775633063
Validation loss per 100 evaluation steps: 0.8519212483023092
Validation loss per 100 evaluation steps: 0.8496041964005651
Validation Loss: 0.848398415123423
Validation Accuracy: 0.6647092490842491
Overall accuracy: 0.6647312788447224
Overall loss: 0.848398415123423
Training Epoch: 2
Training loss epoch: 0.5332267880439758
Training accuracy epoch: 0.6596540178571428
Overall acc: 0.6596873255164712
Overall loss: 358.3283996582031
Validation loss per 100 evaluation steps: 1.3254857063293457
Validation loss per 100 evaluation steps: 0.8855965004699065
Validation loss per 100 evaluation steps: 0.8890784207861222
Validation loss per 100 evaluation steps: 0.8861662173191971
Validation loss per 100 evaluation steps: 0.8931723292777664
Validation Loss: 0.8962196627543086
Validation Accuracy: 0.6578296703296703
Overall accuracy: 0.6578829834747655
Overall loss: 0.8962196627543086
Training Epoch: 3
Training loss epoch: 0.49056676030158997
Training accuracy epoch: 0.6990327380952381
Overall acc: 0.6990042806625721
Overall loss: 329.6608581542969
Validation loss per 100 evaluation steps: 1.0270154476165771
Validation loss per 100 evaluation steps: 0.6695555936139409
Validation loss per 100 evaluation steps: 0.7242374503568038
Validation loss per 100 evaluation steps: 0.7187371356196182
Validation loss per 100 evaluation steps: 0.7124645481643237
Validation Loss: 0.7124517819178956
Validation Accuracy: 0.7581158424908425
Overall accuracy: 0.75807652225696
Overall loss: 0.7124517819178956
Training Epoch: 4
Training loss epoch: 0.4707324206829071
Training accuracy epoch: 0.7240513392857143
Overall acc: 0.724036850921273
Overall loss: 316.3321838378906
Validation loss per 100 evaluation steps: 0.23085938394069672
Validation loss per 100 evaluation steps: 0.7662520222734697
Validation loss per 100 evaluation steps: 0.7752023778892868
Validation loss per 100 evaluation steps: 0.7818675240210916
Validation loss per 100 evaluation steps: 0.7917829419386357
Validation Loss: 0.7912519400673254
Validation Accuracy: 0.7314445970695971
Overall accuracy: 0.7314277207086497
Overall loss: 0.7912519400673254
Training Epoch: 5
Training loss epoch: 0.41657742857933044
Training accuracy epoch: 0.7586030505952381
Overall acc: 0.758607854085241
Overall loss: 279.9400329589844
Validation loss per 100 evaluation steps: 1.2695953845977783
Validation loss per 100 evaluation steps: 0.7660245473432069
Validation loss per 100 evaluation steps: 0.7486078429281415
Validation loss per 100 evaluation steps: 0.7527413615752693
Validation loss per 100 evaluation steps: 0.7410841208517998
Validation Loss: 0.7378415731447083
Validation Accuracy: 0.747779304029304
Overall accuracy: 0.7478040792020247
Overall loss: 0.7378415731447083
Training Epoch: 6
Training loss epoch: 0.3735285699367523
Training accuracy epoch: 0.7753348214285714
Overall acc: 0.7754048017867113
Overall loss: 251.01119995117188
Validation loss per 100 evaluation steps: 0.9747512340545654
Validation loss per 100 evaluation steps: 0.794707441093898
Validation loss per 100 evaluation steps: 0.7691148446122212
Validation loss per 100 evaluation steps: 0.7680137061894534
Validation loss per 100 evaluation steps: 0.786625101307681
Validation Loss: 0.7858362946127142
Validation Accuracy: 0.731856684981685
Overall accuracy: 0.73187434866756
Overall loss: 0.7858362946127142
Early stopping
### Valid set performance:
Validation loss per 100 evaluation steps: 1.2337496280670166
Validation loss per 100 evaluation steps: 0.7608783531306994
Validation loss per 100 evaluation steps: 0.7622042740903684
Validation loss per 100 evaluation steps: 0.7681967080431523
Validation Loss: 0.7734752545754114
Validation Accuracy: 0.73159913003663
Overall accuracy: 0.7316210683044854
Overall loss: 0.7734752545754114
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       284
       BEFORE       0.96      0.75      0.84      3870
BEFOREOVERLAP       0.29      0.37      0.33       140
      OVERLAP       0.45      0.90      0.60      1079

     accuracy                           0.73      5373
    macro avg       0.42      0.51      0.44      5373
 weighted avg       0.79      0.73      0.74      5373

### Summary
               precision    recall  f1-score   support

       BEFORE       0.96      0.75      0.84      3870
      OVERLAP       0.45      0.90      0.60      1079
        AFTER       0.00      0.00      0.00       284
BEFOREOVERLAP       0.29      0.37      0.33       140

     accuracy                           0.73      5373
    macro avg       0.42      0.51      0.44      5373
 weighted avg       0.79      0.73      0.74      5373

### Test set performance:
Validation loss per 100 evaluation steps: 0.5484285354614258
Validation loss per 100 evaluation steps: 0.8089570724728083
Validation loss per 100 evaluation steps: 0.7890643315083945
Validation loss per 100 evaluation steps: 0.7936330017853417
Validation loss per 100 evaluation steps: 0.7816122328848613
Validation Loss: 0.7857030237004871
Validation Accuracy: 0.7319253663003663
Overall accuracy: 0.73187434866756
Overall loss: 0.7857030237004871
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       363
       BEFORE       0.95      0.76      0.84      4877
BEFOREOVERLAP       0.25      0.34      0.28       160
      OVERLAP       0.45      0.89      0.59      1317

     accuracy                           0.73      6717
    macro avg       0.41      0.50      0.43      6717
 weighted avg       0.79      0.73      0.74      6717

### Summary
               precision    recall  f1-score   support

       BEFORE       0.95      0.76      0.84      4877
      OVERLAP       0.45      0.89      0.59      1317
        AFTER       0.00      0.00      0.00       363
BEFOREOVERLAP       0.25      0.34      0.28       160

     accuracy                           0.73      6717
    macro avg       0.41      0.50      0.43      6717
 weighted avg       0.79      0.73      0.74      6717

Finished with this task.
###### (3) Training for configuration file: a-bilstm.ini
LOAD False
Using: cuda:0 with NN
{'BEFORE': 13827, 'OVERLAP': 4112, 'BEFOREOVERLAP': 491, 'AFTER': 1113}
{'BEFORE': 0.3533485210096189, 'OVERLAP': 1.188168774319066, 'BEFOREOVERLAP': 9.95061099796334, 'AFTER': 4.389712488769092}
Create sweep with ID: 2g0fh6zo
Sweep URL: https://wandb.ai/julievt-ntnu/tre-dtr-a-Task.SEQUENCE-nn-model/sweeps/2g0fh6zo
{'batch_size': 32, 'early_stopping_delta': 0.01, 'early_stopping_patience': 3, 'embedding_dim': 32, 'epochs': 100, 'learning_rate': 0.0001, 'max_length': 64, 'num_workers': 0, 'optimizer': 'adam', 'shuffle': True, 'valid_batch_size': 16, 'weight_decay': 0}
Epoch 1
-------------------------------
Batch 0, Loss: 1.360456943511963
Batch 100, Loss: 1.0819777250289917
Batch 200, Loss: 1.0181539058685303
Batch 300, Loss: 0.9644308090209961
Epoch 2
-------------------------------
Batch 0, Loss: 0.8414106965065002
Batch 100, Loss: 1.2555214166641235
Batch 200, Loss: 0.7793093919754028
Batch 300, Loss: 0.5588460564613342
Epoch 3
-------------------------------
Batch 0, Loss: 0.8021366000175476
Batch 100, Loss: 0.608971357345581
Batch 200, Loss: 0.622276246547699
Batch 300, Loss: 1.0444748401641846
Epoch 4
-------------------------------
Batch 0, Loss: 0.5747751593589783
Batch 100, Loss: 0.6754841804504395
Batch 200, Loss: 0.570069432258606
Batch 300, Loss: 0.5116301774978638
Epoch 5
-------------------------------
Batch 0, Loss: 0.6799021363258362
Batch 100, Loss: 0.7274573445320129
Batch 200, Loss: 0.5798009634017944
Batch 300, Loss: 0.5995309948921204
Epoch 6
-------------------------------
Batch 0, Loss: 0.7475738525390625
Batch 100, Loss: 0.7053067088127136
Batch 200, Loss: 1.1200793981552124
Batch 300, Loss: 0.48527663946151733
Epoch 7
-------------------------------
Batch 0, Loss: 1.1524946689605713
Batch 100, Loss: 0.8717202544212341
Batch 200, Loss: 0.32016950845718384
Batch 300, Loss: 0.46346911787986755
Epoch 8
-------------------------------
Batch 0, Loss: 0.6937345266342163
Batch 100, Loss: 0.796217143535614
Batch 200, Loss: 0.6002622842788696
Batch 300, Loss: 0.5079167485237122
Epoch 9
-------------------------------
Batch 0, Loss: 0.5443073511123657
Batch 100, Loss: 0.5528795719146729
Batch 200, Loss: 0.6802259683609009
Batch 300, Loss: 0.6392069458961487
Epoch 10
-------------------------------
Batch 0, Loss: 0.5031555891036987
Batch 100, Loss: 0.6756507754325867
Batch 200, Loss: 0.6868047118186951
Batch 300, Loss: 0.5901394486427307
Epoch 11
-------------------------------
Batch 0, Loss: 0.8934387564659119
Batch 100, Loss: 0.732675313949585
Batch 200, Loss: 0.4040302038192749
Batch 300, Loss: 0.779143750667572
Epoch 12
-------------------------------
Batch 0, Loss: 0.28980180621147156
Batch 100, Loss: 0.6241210699081421
Batch 200, Loss: 0.5425596237182617
Batch 300, Loss: 0.6608206033706665
Epoch 13
-------------------------------
Batch 0, Loss: 0.7230453491210938
Batch 100, Loss: 0.6974928379058838
Batch 200, Loss: 0.4667913019657135
Batch 300, Loss: 0.8425576090812683
Epoch 14
-------------------------------
Batch 0, Loss: 0.3192722499370575
Batch 100, Loss: 0.46326372027397156
Batch 200, Loss: 0.49095675349235535
Batch 300, Loss: 0.6282104253768921
Epoch 15
-------------------------------
Batch 0, Loss: 0.4791792035102844
Batch 100, Loss: 0.27919161319732666
Batch 200, Loss: 0.8640414476394653
Batch 300, Loss: 0.3550618588924408
Epoch 16
-------------------------------
Batch 0, Loss: 0.5574889779090881
Batch 100, Loss: 0.4798281192779541
Batch 200, Loss: 0.37137529253959656
Batch 300, Loss: 0.890704870223999
Early stopping
### Valid set performance:
Validation Accuracy: 0.6229613047649505
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       175
       BEFORE       0.96      0.60      0.74      2197
BEFOREOVERLAP       0.00      0.00      0.00        87
      OVERLAP       0.36      0.95      0.52       668

     accuracy                           0.62      3127
    macro avg       0.33      0.39      0.31      3127
 weighted avg       0.75      0.62      0.63      3127

### Summary
               precision    recall  f1-score   support

       BEFORE       0.96      0.60      0.74      2197
      OVERLAP       0.36      0.95      0.52       668
        AFTER       0.00      0.00      0.00       175
BEFOREOVERLAP       0.00      0.00      0.00        87

     accuracy                           0.62      3127
    macro avg       0.33      0.39      0.31      3127
 weighted avg       0.75      0.62      0.63      3127

### Test set performance:
Validation Accuracy: 0.6334100793041698
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       222
       BEFORE       0.96      0.61      0.75      2776
BEFOREOVERLAP       0.00      0.00      0.00        84
      OVERLAP       0.36      0.95      0.53       827

     accuracy                           0.63      3909
    macro avg       0.33      0.39      0.32      3909
 weighted avg       0.76      0.63      0.64      3909

### Summary
               precision    recall  f1-score   support

       BEFORE       0.96      0.61      0.75      2776
      OVERLAP       0.36      0.95      0.53       827
        AFTER       0.00      0.00      0.00       222
BEFOREOVERLAP       0.00      0.00      0.00        84

     accuracy                           0.63      3909
    macro avg       0.33      0.39      0.32      3909
 weighted avg       0.76      0.63      0.64      3909

{'batch_size': 32, 'early_stopping_delta': 0.01, 'early_stopping_patience': 3, 'embedding_dim': 32, 'epochs': 100, 'learning_rate': 0.0001, 'max_length': 64, 'num_workers': 0, 'optimizer': 'adam', 'shuffle': True, 'valid_batch_size': 16, 'weight_decay': 1e-05}
Epoch 1
-------------------------------
Batch 0, Loss: 1.442196011543274
Batch 100, Loss: 1.2891303300857544
Batch 200, Loss: 1.0977129936218262
Batch 300, Loss: 1.168634295463562
Epoch 2
-------------------------------
Batch 0, Loss: 0.6141809225082397
Batch 100, Loss: 1.0379035472869873
Batch 200, Loss: 0.8403725028038025
Batch 300, Loss: 0.6148109436035156
Epoch 3
-------------------------------
Batch 0, Loss: 0.6387720704078674
Batch 100, Loss: 0.70566326379776
Batch 200, Loss: 0.5399424433708191
Batch 300, Loss: 0.6055605411529541
Epoch 4
-------------------------------
Batch 0, Loss: 0.5723464488983154
Batch 100, Loss: 0.483938992023468
Batch 200, Loss: 0.6767107844352722
Batch 300, Loss: 0.4000987112522125
Epoch 5
-------------------------------
Batch 0, Loss: 0.6945923566818237
Batch 100, Loss: 0.6355313658714294
Batch 200, Loss: 0.8772009015083313
Batch 300, Loss: 0.9268844723701477
Epoch 6
-------------------------------
Batch 0, Loss: 1.0327116250991821
Batch 100, Loss: 0.6273854970932007
Batch 200, Loss: 0.39043185114860535
Batch 300, Loss: 0.48510491847991943
Epoch 7
-------------------------------
Batch 0, Loss: 0.8598272800445557
Batch 100, Loss: 0.4241963028907776
Batch 200, Loss: 0.5590304732322693
Batch 300, Loss: 0.5926529765129089
Epoch 8
-------------------------------
Batch 0, Loss: 0.5315079689025879
Batch 100, Loss: 0.5130481719970703
Batch 200, Loss: 0.9739871025085449
Batch 300, Loss: 0.4381019175052643
Epoch 9
-------------------------------
Batch 0, Loss: 0.6709577441215515
Batch 100, Loss: 0.5773423314094543
Batch 200, Loss: 0.8435250520706177
Batch 300, Loss: 0.4950222373008728
Epoch 10
-------------------------------
Batch 0, Loss: 0.7073625922203064
Batch 100, Loss: 0.4082857072353363
Batch 200, Loss: 0.4860103130340576
Batch 300, Loss: 0.534344494342804
Epoch 11
-------------------------------
Batch 0, Loss: 0.7371265292167664
Batch 100, Loss: 0.6027767062187195
Batch 200, Loss: 1.110787272453308
Batch 300, Loss: 0.9338398575782776
Epoch 12
-------------------------------
Batch 0, Loss: 0.39440786838531494
Batch 100, Loss: 0.6416289210319519
Batch 200, Loss: 0.7418439984321594
Batch 300, Loss: 0.47564294934272766
Epoch 13
-------------------------------
Batch 0, Loss: 0.6315852403640747
Batch 100, Loss: 0.42135319113731384
Batch 200, Loss: 0.3711175322532654
Batch 300, Loss: 0.7242725491523743
Epoch 14
-------------------------------
Batch 0, Loss: 0.6093767285346985
Batch 100, Loss: 0.4249826669692993
Batch 200, Loss: 0.6273354291915894
Batch 300, Loss: 0.5295241475105286
Epoch 15
-------------------------------
Batch 0, Loss: 0.5623834133148193
Batch 100, Loss: 0.5115863680839539
Batch 200, Loss: 0.34731927514076233
Batch 300, Loss: 0.9955660104751587
Epoch 16
-------------------------------
Batch 0, Loss: 0.9603821039199829
Batch 100, Loss: 0.4527278244495392
Batch 200, Loss: 0.4069010317325592
Batch 300, Loss: 0.3632007837295532
Epoch 17
-------------------------------
Batch 0, Loss: 1.0087960958480835
Batch 100, Loss: 0.6424108147621155
Batch 200, Loss: 0.6201233863830566
Batch 300, Loss: 0.5447495579719543
Epoch 18
-------------------------------
Batch 0, Loss: 0.6343851685523987
Batch 100, Loss: 0.5304193496704102
Batch 200, Loss: 0.49243831634521484
Batch 300, Loss: 0.6405119299888611
Epoch 19
-------------------------------
Batch 0, Loss: 0.43801411986351013
Batch 100, Loss: 0.5296105146408081
Batch 200, Loss: 0.550918698310852
Batch 300, Loss: 0.5430592894554138
Epoch 20
-------------------------------
Batch 0, Loss: 0.7285453677177429
Batch 100, Loss: 0.4833109378814697
Batch 200, Loss: 0.5393949151039124
Batch 300, Loss: 0.5479886531829834
Epoch 21
-------------------------------
Batch 0, Loss: 0.38823163509368896
Batch 100, Loss: 0.5367892980575562
Batch 200, Loss: 0.5361023545265198
Batch 300, Loss: 0.29762575030326843
Epoch 22
-------------------------------
Batch 0, Loss: 0.5733593106269836
Batch 100, Loss: 0.6660970449447632
Batch 200, Loss: 0.9364190101623535
Batch 300, Loss: 0.5278672575950623
Epoch 23
-------------------------------
Batch 0, Loss: 0.4906894564628601
Batch 100, Loss: 0.3999912738800049
Batch 200, Loss: 0.6743557453155518
Batch 300, Loss: 0.2820068597793579
Epoch 24
-------------------------------
Batch 0, Loss: 0.6705369353294373
Batch 100, Loss: 0.5265612602233887
Batch 200, Loss: 0.3368145823478699
Batch 300, Loss: 0.5745847225189209
Epoch 25
-------------------------------
Batch 0, Loss: 0.5104588270187378
Batch 100, Loss: 0.3579283058643341
Batch 200, Loss: 0.5636382699012756
Batch 300, Loss: 0.32396286725997925
Epoch 26
-------------------------------
Batch 0, Loss: 0.3747415840625763
Batch 100, Loss: 0.6081175208091736
Batch 200, Loss: 0.28393545746803284
Batch 300, Loss: 0.3613184690475464
Epoch 27
-------------------------------
Batch 0, Loss: 0.4192233383655548
Batch 100, Loss: 0.4218873381614685
Batch 200, Loss: 0.4625336527824402
Batch 300, Loss: 0.9193309545516968
Epoch 28
-------------------------------
Batch 0, Loss: 0.672755777835846
Batch 100, Loss: 0.4493173658847809
Batch 200, Loss: 0.4865488111972809
Batch 300, Loss: 0.4034807085990906
Epoch 29
-------------------------------
Batch 0, Loss: 0.6610369086265564
Batch 100, Loss: 0.3580639958381653
Batch 200, Loss: 0.6589713096618652
Batch 300, Loss: 0.30306610465049744
Epoch 30
-------------------------------
Batch 0, Loss: 0.45764094591140747
Batch 100, Loss: 0.46406301856040955
Batch 200, Loss: 0.29688185453414917
Batch 300, Loss: 0.5729725360870361
Epoch 31
-------------------------------
Batch 0, Loss: 0.3338054418563843
Batch 100, Loss: 0.5067046880722046
Batch 200, Loss: 0.5704329609870911
Batch 300, Loss: 0.5105952024459839
Epoch 32
-------------------------------
Batch 0, Loss: 0.6656323075294495
Batch 100, Loss: 0.47857391834259033
Batch 200, Loss: 0.7674328684806824
Batch 300, Loss: 0.4305558204650879
Epoch 33
-------------------------------
Batch 0, Loss: 0.6954570412635803
Batch 100, Loss: 0.45620453357696533
Batch 200, Loss: 0.2823728322982788
Batch 300, Loss: 0.565630316734314
Epoch 34
-------------------------------
Batch 0, Loss: 0.7045255303382874
Batch 100, Loss: 0.5975692868232727
Batch 200, Loss: 0.5369694232940674
Batch 300, Loss: 0.6105388402938843
Epoch 35
-------------------------------
Batch 0, Loss: 0.28778401017189026
Batch 100, Loss: 0.5152580738067627
Batch 200, Loss: 0.7981494665145874
Batch 300, Loss: 0.4552261531352997
Epoch 36
-------------------------------
Batch 0, Loss: 0.4377157688140869
Batch 100, Loss: 0.2846912145614624
Batch 200, Loss: 0.9744075536727905
Batch 300, Loss: 0.30052685737609863
Epoch 37
-------------------------------
Batch 0, Loss: 0.5195634365081787
Batch 100, Loss: 0.40895339846611023
Batch 200, Loss: 0.6172151565551758
Batch 300, Loss: 0.25264137983322144
Epoch 38
-------------------------------
Batch 0, Loss: 0.5185317993164062
Batch 100, Loss: 0.34197866916656494
Batch 200, Loss: 0.42482906579971313
Batch 300, Loss: 0.584476888179779
Epoch 39
-------------------------------
Batch 0, Loss: 0.6641905903816223
Batch 100, Loss: 0.36741650104522705
Batch 200, Loss: 0.5306792855262756
Batch 300, Loss: 0.5942761301994324
Epoch 40
-------------------------------
Batch 0, Loss: 0.47886377573013306
Batch 100, Loss: 0.24458777904510498
Batch 200, Loss: 0.5069913268089294
Batch 300, Loss: 0.633487343788147
Epoch 41
-------------------------------
Batch 0, Loss: 0.378918319940567
Batch 100, Loss: 0.4809268116950989
Batch 200, Loss: 0.6894281506538391
Batch 300, Loss: 0.41480404138565063
Epoch 42
-------------------------------
Batch 0, Loss: 0.4460970163345337
Batch 100, Loss: 0.29520097374916077
Batch 200, Loss: 0.5118502378463745
Batch 300, Loss: 0.250878244638443
Epoch 43
-------------------------------
Batch 0, Loss: 0.4446687400341034
Batch 100, Loss: 0.4498278498649597
Batch 200, Loss: 0.44059839844703674
Batch 300, Loss: 0.3113255798816681
Early stopping
### Valid set performance:
Validation Accuracy: 0.6955548448992644
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       175
       BEFORE       0.95      0.71      0.81      2197
BEFOREOVERLAP       0.00      0.00      0.00        87
      OVERLAP       0.42      0.94      0.58       668

     accuracy                           0.70      3127
    macro avg       0.34      0.41      0.35      3127
 weighted avg       0.76      0.70      0.69      3127

### Summary
               precision    recall  f1-score   support

       BEFORE       0.95      0.71      0.81      2197
      OVERLAP       0.42      0.94      0.58       668
        AFTER       0.00      0.00      0.00       175
BEFOREOVERLAP       0.00      0.00      0.00        87

     accuracy                           0.70      3127
    macro avg       0.34      0.41      0.35      3127
 weighted avg       0.76      0.70      0.69      3127

### Test set performance:
Validation Accuracy: 0.6942952161678179
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       222
       BEFORE       0.94      0.70      0.81      2776
BEFOREOVERLAP       0.00      0.00      0.00        84
      OVERLAP       0.41      0.92      0.57       827

     accuracy                           0.69      3909
    macro avg       0.34      0.41      0.34      3909
 weighted avg       0.76      0.69      0.69      3909

### Summary
               precision    recall  f1-score   support

       BEFORE       0.94      0.70      0.81      2776
      OVERLAP       0.41      0.92      0.57       827
        AFTER       0.00      0.00      0.00       222
BEFOREOVERLAP       0.00      0.00      0.00        84

     accuracy                           0.69      3909
    macro avg       0.34      0.41      0.34      3909
 weighted avg       0.76      0.69      0.69      3909

{'batch_size': 32, 'early_stopping_delta': 0.01, 'early_stopping_patience': 3, 'embedding_dim': 32, 'epochs': 100, 'learning_rate': 0.0001, 'max_length': 64, 'num_workers': 0, 'optimizer': 'adam', 'shuffle': True, 'valid_batch_size': 16, 'weight_decay': 0.0001}
Epoch 1
-------------------------------
Batch 0, Loss: 1.3441557884216309
Batch 100, Loss: 1.2408090829849243
Batch 200, Loss: 1.0068234205245972
Batch 300, Loss: 1.0532699823379517
Epoch 2
-------------------------------
Batch 0, Loss: 1.0667483806610107
Batch 100, Loss: 1.2014514207839966
Batch 200, Loss: 0.9448300004005432
Batch 300, Loss: 0.7528390288352966
Epoch 3
-------------------------------
Batch 0, Loss: 0.7339250445365906
Batch 100, Loss: 0.9476674199104309
Batch 200, Loss: 0.5134328007698059
Batch 300, Loss: 1.2145322561264038
Epoch 4
-------------------------------
Batch 0, Loss: 0.6332681775093079
Batch 100, Loss: 0.5292725563049316
Batch 200, Loss: 0.4746928811073303
Batch 300, Loss: 0.9392610192298889
Epoch 5
-------------------------------
Batch 0, Loss: 0.605806827545166
Batch 100, Loss: 0.5886576175689697
Batch 200, Loss: 0.7220278978347778
Batch 300, Loss: 0.6440502405166626
Epoch 6
-------------------------------
Batch 0, Loss: 0.5948271751403809
Batch 100, Loss: 0.3823870122432709
Batch 200, Loss: 0.7623834609985352
Batch 300, Loss: 0.46492257714271545
Epoch 7
-------------------------------
Batch 0, Loss: 0.5587981939315796
Batch 100, Loss: 0.4824486970901489
Batch 200, Loss: 0.3621315062046051
Batch 300, Loss: 0.6780751347541809
Epoch 8
-------------------------------
Batch 0, Loss: 0.7394517064094543
Batch 100, Loss: 0.7838788032531738
Batch 200, Loss: 0.48302140831947327
Batch 300, Loss: 0.5107328295707703
Epoch 9
-------------------------------
Batch 0, Loss: 0.4648788869380951
Batch 100, Loss: 0.726578950881958
Batch 200, Loss: 0.830329179763794
Batch 300, Loss: 0.6170598268508911
Epoch 10
-------------------------------
Batch 0, Loss: 0.492466002702713
Batch 100, Loss: 0.3762330710887909
Batch 200, Loss: 0.5792024731636047
Batch 300, Loss: 0.5299148559570312
Epoch 11
-------------------------------
Batch 0, Loss: 0.6838040947914124
Batch 100, Loss: 0.4538216292858124
Batch 200, Loss: 0.6161211133003235
Batch 300, Loss: 0.681904137134552
Epoch 12
-------------------------------
Batch 0, Loss: 0.9013572931289673
Batch 100, Loss: 0.6178596019744873
Batch 200, Loss: 0.6332347989082336
Batch 300, Loss: 0.5781309008598328
Epoch 13
-------------------------------
Batch 0, Loss: 0.6958684325218201
Batch 100, Loss: 0.5299181342124939
Batch 200, Loss: 0.7343679666519165
Batch 300, Loss: 0.7362791895866394
Epoch 14
-------------------------------
Batch 0, Loss: 0.4671674370765686
Batch 100, Loss: 0.5852057337760925
Batch 200, Loss: 0.6852794289588928
Batch 300, Loss: 0.5147956609725952
Epoch 15
-------------------------------
Batch 0, Loss: 0.390458881855011
Batch 100, Loss: 0.44328197836875916
Batch 200, Loss: 0.4937112033367157
Batch 300, Loss: 0.5428100824356079
Epoch 16
-------------------------------
Batch 0, Loss: 0.42600011825561523
Batch 100, Loss: 0.7675115466117859
Batch 200, Loss: 0.46932879090309143
Batch 300, Loss: 0.3594474494457245
Epoch 17
-------------------------------
Batch 0, Loss: 0.564443826675415
Batch 100, Loss: 0.5501323938369751
Batch 200, Loss: 0.3200368881225586
Batch 300, Loss: 0.40394896268844604
Epoch 18
-------------------------------
Batch 0, Loss: 0.43446487188339233
Batch 100, Loss: 1.146020770072937
Batch 200, Loss: 0.49370893836021423
Batch 300, Loss: 0.6381887793540955
Epoch 19
-------------------------------
Batch 0, Loss: 0.5126928091049194
Batch 100, Loss: 0.6688619256019592
Batch 200, Loss: 0.55026775598526
Batch 300, Loss: 0.6128672957420349
Epoch 20
-------------------------------
Batch 0, Loss: 0.9470910429954529
Batch 100, Loss: 0.4587518870830536
Batch 200, Loss: 0.6915278434753418
Batch 300, Loss: 0.5891782641410828
Epoch 21
-------------------------------
Batch 0, Loss: 0.6158595085144043
Batch 100, Loss: 0.36858242750167847
Batch 200, Loss: 0.7220831513404846
Batch 300, Loss: 0.7032734751701355
Epoch 22
-------------------------------
Batch 0, Loss: 0.4502870440483093
Batch 100, Loss: 0.595653235912323
Batch 200, Loss: 0.7493184804916382
Batch 300, Loss: 0.8853940367698669
Epoch 23
-------------------------------
Batch 0, Loss: 0.4713592529296875
Batch 100, Loss: 0.3489128351211548
Batch 200, Loss: 0.5444828867912292
Batch 300, Loss: 0.6215187311172485
Epoch 24
-------------------------------
Batch 0, Loss: 0.539050817489624
Batch 100, Loss: 0.7219162583351135
Batch 200, Loss: 0.4511202573776245
Batch 300, Loss: 0.6432204842567444
Epoch 25
-------------------------------
Batch 0, Loss: 0.7643365859985352
Batch 100, Loss: 0.48472312092781067
Batch 200, Loss: 0.6171125769615173
Batch 300, Loss: 0.8171852827072144
Early stopping
### Valid set performance:
Validation Accuracy: 0.631595778701631
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       175
       BEFORE       0.95      0.62      0.75      2197
BEFOREOVERLAP       0.00      0.00      0.00        87
      OVERLAP       0.37      0.93      0.52       668

     accuracy                           0.63      3127
    macro avg       0.33      0.39      0.32      3127
 weighted avg       0.74      0.63      0.64      3127

### Summary
               precision    recall  f1-score   support

       BEFORE       0.95      0.62      0.75      2197
      OVERLAP       0.37      0.93      0.52       668
        AFTER       0.00      0.00      0.00       175
BEFOREOVERLAP       0.00      0.00      0.00        87

     accuracy                           0.63      3127
    macro avg       0.33      0.39      0.32      3127
 weighted avg       0.74      0.63      0.64      3127

### Test set performance:
Validation Accuracy: 0.6477359938603223
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       222
       BEFORE       0.96      0.63      0.76      2776
BEFOREOVERLAP       0.00      0.00      0.00        84
      OVERLAP       0.37      0.95      0.54       827

     accuracy                           0.65      3909
    macro avg       0.33      0.39      0.32      3909
 weighted avg       0.76      0.65      0.65      3909

### Summary
               precision    recall  f1-score   support

       BEFORE       0.96      0.63      0.76      2776
      OVERLAP       0.37      0.95      0.54       827
        AFTER       0.00      0.00      0.00       222
BEFOREOVERLAP       0.00      0.00      0.00        84

     accuracy                           0.65      3909
    macro avg       0.33      0.39      0.32      3909
 weighted avg       0.76      0.65      0.65      3909

{'batch_size': 32, 'early_stopping_delta': 0.01, 'early_stopping_patience': 3, 'embedding_dim': 32, 'epochs': 100, 'learning_rate': 0.0001, 'max_length': 64, 'num_workers': 0, 'optimizer': 'adam', 'shuffle': True, 'valid_batch_size': 16, 'weight_decay': 0.001}
Epoch 1
-------------------------------
Batch 0, Loss: 1.3403533697128296
Batch 100, Loss: 1.2335294485092163
Batch 200, Loss: 0.9443355798721313
Batch 300, Loss: 0.8555939197540283
Epoch 2
-------------------------------
Batch 0, Loss: 0.9754050970077515
Batch 100, Loss: 0.7909338474273682
Batch 200, Loss: 0.7807570695877075
Batch 300, Loss: 0.5773420929908752
Epoch 3
-------------------------------
Batch 0, Loss: 0.7133545875549316
Batch 100, Loss: 0.7159677147865295
Batch 200, Loss: 0.63343745470047
Batch 300, Loss: 0.5841800570487976
Epoch 4
-------------------------------
Batch 0, Loss: 0.5526745915412903
Batch 100, Loss: 0.5069718956947327
Batch 200, Loss: 0.70610511302948
Batch 300, Loss: 0.5724546313285828
Epoch 5
-------------------------------
Batch 0, Loss: 0.45290496945381165
Batch 100, Loss: 0.5459970831871033
Batch 200, Loss: 0.44171515107154846
Batch 300, Loss: 0.5485280156135559
Epoch 6
-------------------------------
Batch 0, Loss: 0.579658567905426
Batch 100, Loss: 0.41704869270324707
Batch 200, Loss: 0.7748945355415344
Batch 300, Loss: 0.7528266906738281
Epoch 7
-------------------------------
Batch 0, Loss: 0.6223716139793396
Batch 100, Loss: 0.8626539707183838
Batch 200, Loss: 0.3916214108467102
Batch 300, Loss: 0.4636443555355072
Epoch 8
-------------------------------
Batch 0, Loss: 0.44194260239601135
Batch 100, Loss: 0.8333380222320557
Batch 200, Loss: 0.5054104328155518
Batch 300, Loss: 0.6661995053291321
Epoch 9
-------------------------------
Batch 0, Loss: 0.35989633202552795
Batch 100, Loss: 0.7463723421096802
Batch 200, Loss: 0.5936931371688843
Batch 300, Loss: 0.4840938150882721
Epoch 10
-------------------------------
Batch 0, Loss: 0.6986163258552551
Batch 100, Loss: 0.4812107980251312
Batch 200, Loss: 0.5313414335250854
Batch 300, Loss: 0.6515248417854309
Epoch 11
-------------------------------
Batch 0, Loss: 0.39601224660873413
Batch 100, Loss: 0.5276229977607727
Batch 200, Loss: 0.7139753103256226
Batch 300, Loss: 0.40352755784988403
Epoch 12
-------------------------------
Batch 0, Loss: 0.6489803194999695
Batch 100, Loss: 0.6574989557266235
Batch 200, Loss: 0.49200254678726196
Batch 300, Loss: 0.5167492628097534
Epoch 13
-------------------------------
Batch 0, Loss: 0.4833749830722809
Batch 100, Loss: 0.6623618602752686
Batch 200, Loss: 0.5205492973327637
Batch 300, Loss: 0.41753730177879333
Epoch 14
-------------------------------
Batch 0, Loss: 0.659316897392273
Batch 100, Loss: 0.3722302317619324
Batch 200, Loss: 0.5216646790504456
Batch 300, Loss: 0.791999340057373
Epoch 15
-------------------------------
Batch 0, Loss: 0.6269014477729797
Batch 100, Loss: 0.5388408303260803
Batch 200, Loss: 0.3975718319416046
Batch 300, Loss: 0.4340285658836365
Epoch 16
-------------------------------
Batch 0, Loss: 0.3944988250732422
Batch 100, Loss: 0.4090595543384552
Batch 200, Loss: 0.8017602562904358
Batch 300, Loss: 0.44545236229896545
Epoch 17
-------------------------------
Batch 0, Loss: 0.5014697313308716
Batch 100, Loss: 0.670268714427948
Batch 200, Loss: 0.8380653858184814
Batch 300, Loss: 0.3009871244430542
Epoch 18
-------------------------------
Batch 0, Loss: 0.3840845823287964
Batch 100, Loss: 0.6425232887268066
Batch 200, Loss: 0.8994405269622803
Batch 300, Loss: 0.8480561375617981
Epoch 19
-------------------------------
Batch 0, Loss: 0.42182353138923645
Batch 100, Loss: 0.45995771884918213
Batch 200, Loss: 0.6928803324699402
Batch 300, Loss: 0.4005061089992523
Epoch 20
-------------------------------
Batch 0, Loss: 0.8371137976646423
Batch 100, Loss: 0.6336669325828552
Batch 200, Loss: 0.6627129316329956
Batch 300, Loss: 0.2986893355846405
Epoch 21
-------------------------------
Batch 0, Loss: 0.7036477327346802
Batch 100, Loss: 1.1343588829040527
Batch 200, Loss: 0.3278029263019562
Batch 300, Loss: 0.6195259690284729
Epoch 22
-------------------------------
Batch 0, Loss: 1.0577235221862793
Batch 100, Loss: 0.5981826782226562
Batch 200, Loss: 0.6950089931488037
Batch 300, Loss: 0.33919206261634827
Epoch 23
-------------------------------
Batch 0, Loss: 0.40417712926864624
Batch 100, Loss: 0.4601183533668518
Batch 200, Loss: 0.6785449385643005
Batch 300, Loss: 0.5112245678901672
Epoch 24
-------------------------------
Batch 0, Loss: 0.6608977317810059
Batch 100, Loss: 0.42837613821029663
Batch 200, Loss: 0.4681011736392975
Batch 300, Loss: 0.7256708145141602
Epoch 25
-------------------------------
Batch 0, Loss: 0.45524847507476807
Batch 100, Loss: 0.7835831046104431
Batch 200, Loss: 0.37367528676986694
Batch 300, Loss: 0.5097325444221497
Epoch 26
-------------------------------
Batch 0, Loss: 0.6476969122886658
Batch 100, Loss: 0.4967770278453827
Batch 200, Loss: 0.4886406660079956
Batch 300, Loss: 0.28448885679244995
Epoch 27
-------------------------------
Batch 0, Loss: 0.4471389651298523
Batch 100, Loss: 0.5793027281761169
Batch 200, Loss: 0.3460373878479004
Batch 300, Loss: 0.6291439533233643
Epoch 28
-------------------------------
Batch 0, Loss: 0.6240956783294678
Batch 100, Loss: 0.7955638766288757
Batch 200, Loss: 0.47759732604026794
Batch 300, Loss: 0.5960931181907654
Epoch 29
-------------------------------
Batch 0, Loss: 0.7440052628517151
Batch 100, Loss: 0.3804601728916168
Batch 200, Loss: 0.5436179041862488
Batch 300, Loss: 0.615703821182251
Epoch 30
-------------------------------
Batch 0, Loss: 0.5975004434585571
Batch 100, Loss: 0.4371764659881592
Batch 200, Loss: 0.29556676745414734
Batch 300, Loss: 0.6720463633537292
Epoch 31
-------------------------------
Batch 0, Loss: 0.5542449355125427
Batch 100, Loss: 0.6198254823684692
Batch 200, Loss: 0.660984456539154
Batch 300, Loss: 1.0493613481521606
Epoch 32
-------------------------------
Batch 0, Loss: 0.552431046962738
Batch 100, Loss: 0.375596284866333
Batch 200, Loss: 0.3680734634399414
Batch 300, Loss: 0.5895355343818665
Epoch 33
-------------------------------
Batch 0, Loss: 0.3511904180049896
Batch 100, Loss: 0.5388772487640381
Batch 200, Loss: 0.39151644706726074
Batch 300, Loss: 0.5249512791633606
Epoch 34
-------------------------------
Batch 0, Loss: 0.5311469435691833
Batch 100, Loss: 0.4234069287776947
Batch 200, Loss: 0.3936351537704468
Batch 300, Loss: 0.44976621866226196
Epoch 35
-------------------------------
Batch 0, Loss: 0.5476755499839783
Batch 100, Loss: 0.42237764596939087
Batch 200, Loss: 0.2687360346317291
Batch 300, Loss: 0.46621692180633545
Epoch 36
-------------------------------
Batch 0, Loss: 0.32882294058799744
Batch 100, Loss: 0.35677316784858704
Batch 200, Loss: 0.5242192149162292
Batch 300, Loss: 0.5643984079360962
Epoch 37
-------------------------------
Batch 0, Loss: 0.4100702404975891
Batch 100, Loss: 0.28958871960639954
Batch 200, Loss: 0.48101934790611267
Batch 300, Loss: 0.5439494848251343
Epoch 38
-------------------------------
Batch 0, Loss: 0.3526078760623932
Batch 100, Loss: 0.3269047141075134
Batch 200, Loss: 0.7114105224609375
Batch 300, Loss: 0.4005105495452881
Epoch 39
-------------------------------
Batch 0, Loss: 0.5442137718200684
Batch 100, Loss: 0.6567754149436951
Batch 200, Loss: 0.46288472414016724
Batch 300, Loss: 0.5616252422332764
Early stopping
### Valid set performance:
Validation Accuracy: 0.6466261592580749
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       175
       BEFORE       0.96      0.63      0.76      2197
BEFOREOVERLAP       0.00      0.00      0.00        87
      OVERLAP       0.38      0.95      0.54       668

     accuracy                           0.65      3127
    macro avg       0.33      0.40      0.33      3127
 weighted avg       0.75      0.65      0.65      3127

### Summary
               precision    recall  f1-score   support

       BEFORE       0.96      0.63      0.76      2197
      OVERLAP       0.38      0.95      0.54       668
        AFTER       0.00      0.00      0.00       175
BEFOREOVERLAP       0.00      0.00      0.00        87

     accuracy                           0.65      3127
    macro avg       0.33      0.40      0.33      3127
 weighted avg       0.75      0.65      0.65      3127

### Test set performance:
Validation Accuracy: 0.6423637759017652
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       222
       BEFORE       0.96      0.62      0.76      2776
BEFOREOVERLAP       0.00      0.00      0.00        84
      OVERLAP       0.37      0.95      0.53       827

     accuracy                           0.64      3909
    macro avg       0.33      0.39      0.32      3909
 weighted avg       0.76      0.64      0.65      3909

### Summary
               precision    recall  f1-score   support

       BEFORE       0.96      0.62      0.76      2776
      OVERLAP       0.37      0.95      0.53       827
        AFTER       0.00      0.00      0.00       222
BEFOREOVERLAP       0.00      0.00      0.00        84

     accuracy                           0.64      3909
    macro avg       0.33      0.39      0.32      3909
 weighted avg       0.76      0.64      0.65      3909

{'batch_size': 32, 'early_stopping_delta': 0.01, 'early_stopping_patience': 3, 'embedding_dim': 32, 'epochs': 100, 'learning_rate': 0.0001, 'max_length': 64, 'num_workers': 0, 'optimizer': 'adam', 'shuffle': True, 'valid_batch_size': 16, 'weight_decay': 0.01}
Epoch 1
-------------------------------
Batch 0, Loss: 1.4142098426818848
Batch 100, Loss: 1.190327763557434
Batch 200, Loss: 1.1842668056488037
Batch 300, Loss: 0.904495894908905
Epoch 2
-------------------------------
Batch 0, Loss: 0.9387259483337402
Batch 100, Loss: 0.663712739944458
Batch 200, Loss: 1.308266043663025
Batch 300, Loss: 0.7748902440071106
Epoch 3
-------------------------------
Batch 0, Loss: 1.323788046836853
Batch 100, Loss: 0.9231985211372375
Batch 200, Loss: 0.6799824237823486
Batch 300, Loss: 0.38125768303871155
Epoch 4
-------------------------------
Batch 0, Loss: 0.6229152083396912
Batch 100, Loss: 1.1141026020050049
Batch 200, Loss: 0.49005135893821716
Batch 300, Loss: 0.550164520740509
Epoch 5
-------------------------------
Batch 0, Loss: 0.48437604308128357
Batch 100, Loss: 0.5209553837776184
Batch 200, Loss: 1.0647640228271484
Batch 300, Loss: 0.6097826957702637
Epoch 6
-------------------------------
Batch 0, Loss: 0.9302785992622375
Batch 100, Loss: 0.5355270504951477
Batch 200, Loss: 0.5191899538040161
Batch 300, Loss: 0.3400731682777405
Epoch 7
-------------------------------
Batch 0, Loss: 0.9074059724807739
Batch 100, Loss: 0.5484117269515991
Batch 200, Loss: 1.0319573879241943
Batch 300, Loss: 0.4378727972507477
Epoch 8
-------------------------------
Batch 0, Loss: 0.5777981281280518
Batch 100, Loss: 0.5829150676727295
Batch 200, Loss: 0.6239837408065796
Batch 300, Loss: 0.524319052696228
Epoch 9
-------------------------------
Batch 0, Loss: 0.38981467485427856
Batch 100, Loss: 0.7301219701766968
Batch 200, Loss: 0.5970494151115417
Batch 300, Loss: 0.7399452328681946
Epoch 10
-------------------------------
Batch 0, Loss: 0.8605501651763916
Batch 100, Loss: 0.4238121211528778
Batch 200, Loss: 0.6195052266120911
Batch 300, Loss: 0.572494626045227
Epoch 11
-------------------------------
Batch 0, Loss: 0.4987286925315857
Batch 100, Loss: 0.5377241373062134
Batch 200, Loss: 0.3703845143318176
Batch 300, Loss: 0.4551110863685608
Epoch 12
-------------------------------
Batch 0, Loss: 0.525088369846344
Batch 100, Loss: 0.9173599481582642
Batch 200, Loss: 0.7743656039237976
Batch 300, Loss: 0.5871588587760925
Epoch 13
-------------------------------
Batch 0, Loss: 0.5216716527938843
Batch 100, Loss: 0.4021041989326477
Batch 200, Loss: 0.652737557888031
Batch 300, Loss: 0.3597027361392975
Epoch 14
-------------------------------
Batch 0, Loss: 0.5111278295516968
Batch 100, Loss: 0.4295617640018463
Batch 200, Loss: 0.4759758412837982
Batch 300, Loss: 0.8810089826583862
Epoch 15
-------------------------------
Batch 0, Loss: 0.5539894104003906
Batch 100, Loss: 0.9944319725036621
Batch 200, Loss: 0.47940507531166077
Batch 300, Loss: 0.6390016674995422
Epoch 16
-------------------------------
Batch 0, Loss: 0.6180728673934937
Batch 100, Loss: 0.4561915397644043
Batch 200, Loss: 0.4426700472831726
Batch 300, Loss: 0.4774129092693329
Epoch 17
-------------------------------
Batch 0, Loss: 0.5858325958251953
Batch 100, Loss: 0.6804150342941284
Batch 200, Loss: 0.4408790171146393
Batch 300, Loss: 0.6091455221176147
Epoch 18
-------------------------------
Batch 0, Loss: 0.447999507188797
Batch 100, Loss: 0.4666760563850403
Batch 200, Loss: 0.5284414887428284
Batch 300, Loss: 0.4039846360683441
Epoch 19
-------------------------------
Batch 0, Loss: 0.6728497743606567
Batch 100, Loss: 0.47333455085754395
Batch 200, Loss: 0.6000694036483765
Batch 300, Loss: 0.5684496760368347
Epoch 20
-------------------------------
Batch 0, Loss: 0.6841551065444946
Batch 100, Loss: 0.33396556973457336
Batch 200, Loss: 0.6100940704345703
Batch 300, Loss: 0.5368804335594177
Epoch 21
-------------------------------
Batch 0, Loss: 0.35420048236846924
Batch 100, Loss: 0.6393596529960632
Batch 200, Loss: 0.6626211404800415
Batch 300, Loss: 0.4133264422416687
Epoch 22
-------------------------------
Batch 0, Loss: 0.3901800215244293
Batch 100, Loss: 1.2084739208221436
Batch 200, Loss: 0.5533083081245422
Batch 300, Loss: 0.6276097297668457
Epoch 23
-------------------------------
Batch 0, Loss: 0.630567193031311
Batch 100, Loss: 0.5636570453643799
Batch 200, Loss: 0.5362043976783752
Batch 300, Loss: 0.35715606808662415
Epoch 24
-------------------------------
Batch 0, Loss: 0.4957519769668579
Batch 100, Loss: 0.7611702680587769
Batch 200, Loss: 0.6334745287895203
Batch 300, Loss: 0.422289103269577
Epoch 25
-------------------------------
Batch 0, Loss: 0.4453226923942566
Batch 100, Loss: 0.5210643410682678
Batch 200, Loss: 0.7071025371551514
Batch 300, Loss: 0.4256810247898102
Epoch 26
-------------------------------
Batch 0, Loss: 0.3935931324958801
Batch 100, Loss: 0.35450372099876404
Batch 200, Loss: 0.6626468896865845
Batch 300, Loss: 0.8825991153717041
Epoch 27
-------------------------------
Batch 0, Loss: 0.6574438214302063
Batch 100, Loss: 0.29336172342300415
Batch 200, Loss: 0.5527274012565613
Batch 300, Loss: 0.47847452759742737
Epoch 28
-------------------------------
Batch 0, Loss: 0.6216845512390137
Batch 100, Loss: 0.4700269103050232
Batch 200, Loss: 0.4770561456680298
Batch 300, Loss: 0.545021116733551
Epoch 29
-------------------------------
Batch 0, Loss: 0.35025444626808167
Batch 100, Loss: 0.7676815986633301
Batch 200, Loss: 0.6309493184089661
Batch 300, Loss: 0.6967779397964478
Epoch 30
-------------------------------
Batch 0, Loss: 0.3364451825618744
Batch 100, Loss: 0.6953085660934448
Batch 200, Loss: 0.5499348640441895
Batch 300, Loss: 0.41934189200401306
Epoch 31
-------------------------------
Batch 0, Loss: 0.33980801701545715
Batch 100, Loss: 0.28850308060646057
Batch 200, Loss: 0.6968035101890564
Batch 300, Loss: 0.6622437834739685
Epoch 32
-------------------------------
Batch 0, Loss: 0.46946680545806885
Batch 100, Loss: 0.7033324837684631
Batch 200, Loss: 0.6617472171783447
Batch 300, Loss: 0.660463273525238
Epoch 33
-------------------------------
Batch 0, Loss: 0.3310094177722931
Batch 100, Loss: 0.3550984263420105
Batch 200, Loss: 0.35103410482406616
Batch 300, Loss: 0.6235698461532593
Epoch 34
-------------------------------
Batch 0, Loss: 0.5099925398826599
Batch 100, Loss: 0.32090261578559875
Batch 200, Loss: 0.3440326452255249
Batch 300, Loss: 0.5423420071601868
Epoch 35
-------------------------------
Batch 0, Loss: 0.9307965040206909
Batch 100, Loss: 0.7051514387130737
Batch 200, Loss: 0.3266252875328064
Batch 300, Loss: 0.40249040722846985
Epoch 36
-------------------------------
Batch 0, Loss: 0.30934596061706543
Batch 100, Loss: 0.4371420741081238
Batch 200, Loss: 0.26549333333969116
Batch 300, Loss: 0.3759167492389679
Epoch 37
-------------------------------
Batch 0, Loss: 0.8244810104370117
Batch 100, Loss: 0.8755989074707031
Batch 200, Loss: 0.7838077545166016
Batch 300, Loss: 0.7621773481369019
Epoch 38
-------------------------------
Batch 0, Loss: 0.49253353476524353
Batch 100, Loss: 0.3981497287750244
Batch 200, Loss: 0.39710932970046997
Batch 300, Loss: 0.5347443222999573
Epoch 39
-------------------------------
Batch 0, Loss: 0.38024818897247314
Batch 100, Loss: 0.6837083101272583
Batch 200, Loss: 0.3199453353881836
Batch 300, Loss: 0.4528934359550476
Epoch 40
-------------------------------
Batch 0, Loss: 0.6863611340522766
Batch 100, Loss: 0.4650062620639801
Batch 200, Loss: 0.27076101303100586
Batch 300, Loss: 0.6112245321273804
Early stopping
### Valid set performance:
Validation Accuracy: 0.7064278861528621
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       175
       BEFORE       0.93      0.73      0.82      2197
BEFOREOVERLAP       0.00      0.00      0.00        87
      OVERLAP       0.43      0.90      0.58       668

     accuracy                           0.71      3127
    macro avg       0.34      0.41      0.35      3127
 weighted avg       0.74      0.71      0.70      3127

### Summary
               precision    recall  f1-score   support

       BEFORE       0.93      0.73      0.82      2197
      OVERLAP       0.43      0.90      0.58       668
        AFTER       0.00      0.00      0.00       175
BEFOREOVERLAP       0.00      0.00      0.00        87

     accuracy                           0.71      3127
    macro avg       0.34      0.41      0.35      3127
 weighted avg       0.74      0.71      0.70      3127

### Test set performance:
Validation Accuracy: 0.7081094909183935
### BIO-Scheme
               precision    recall  f1-score   support

        AFTER       0.00      0.00      0.00       222
       BEFORE       0.93      0.73      0.82      2776
BEFOREOVERLAP       0.00      0.00      0.00        84
      OVERLAP       0.43      0.89      0.58       827

     accuracy                           0.71      3909
    macro avg       0.34      0.41      0.35      3909
 weighted avg       0.75      0.71      0.70      3909

### Summary
               precision    recall  f1-score   support

       BEFORE       0.93      0.73      0.82      2776
      OVERLAP       0.43      0.89      0.58       827
        AFTER       0.00      0.00      0.00       222
BEFOREOVERLAP       0.00      0.00      0.00        84

     accuracy                           0.71      3909
    macro avg       0.34      0.41      0.35      3909
 weighted avg       0.75      0.71      0.70      3909

Finished with this task.
Process finished!
