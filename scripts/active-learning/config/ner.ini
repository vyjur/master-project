[GENERAL]
name="ner-b"

[MODEL]
name=TokenBERT
load=true
lexicon=false
schema=BIO

[pretrain]
name=ltg/norbert3-small

[train.parameters]
train_batch_size=128
valid_batch_size=32
epochs=100
learning_rate=1e-3
optimizer=adam
weight_decay=1e-5
early_stopping_patience=3
early_stopping_delta=1e-4
embedding_dim=64
max_length=64
stride=0
shuffle=true
num_workers=0

[tuning]
tune=false
count=10
