[GENERAL]
name="ner-b-n2"

[MODEL]
name=TokenBERT
load=false
max_length=128
lexicon=false
schema=BIO

[pretrain]
name=NbAiLab/nb-bert-base

[train.parameters]
train_batch_size=32
valid_batch_size=16
epochs=10
learning_rate=1e-2
optimizer=adam
weight_decay=1e-4
early_stopping_patience=5
early_stopping_delta=0.01
embedding_dim=300
shuffle=true
num_workers=0

[tuning]
tune=false
count=5
